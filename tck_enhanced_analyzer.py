"""
TCK å¢å¼·çµ±è¨ˆæ¸¬è©¦è…³æœ¬ - åŒ…å«è¨˜æ†¶é«”å’Œ I/O çµ±è¨ˆçš„ä¸­æ–‡ç‰ˆæœ¬
"""

import time
import json
import importlib.util
import argparse
import os
import gc
import inspect
import math
from pathlib import Path
from typing import List, Dict, Any, Optional

# ç¨‹å¼ç¢¼å“è³ªåˆ†æåº«å°å…¥
try:
    from radon.visitors import ComplexityVisitor
    from radon.metrics import mi_visit
    from radon.raw import analyze as analyze_raw

    RADON_AVAILABLE = True
except ImportError:
    RADON_AVAILABLE = False
    ComplexityVisitor = None
    mi_visit = None
    analyze_raw = None
    print("âš ï¸  å®‰è£ radon ä»¥é€²è¡Œç¨‹å¼ç¢¼å“è³ªåˆ†æ: pip install radon")

# ç³»çµ±ç›£æ§åº«å°å…¥
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    psutil = None
    PSUTIL_AVAILABLE = False
    print("âš ï¸  å»ºè­°å®‰è£ psutil: pip install psutil")

try:
    import GPUtil

    GPU_AVAILABLE = True
except ImportError:
    GPUtil = None
    GPU_AVAILABLE = False
    print("âš ï¸  å®‰è£ GPUtil ä»¥ç›£æ§ GPU: pip install gputil")

import threading
import platform

SYSTEM_INFO_AVAILABLE = True


class TCKEnhancedAnalyzer:
    """TCK å¢å¼·çµ±è¨ˆåˆ†æå™¨ - åŒ…å«å®Œæ•´çš„ç³»çµ±è³‡æºç›£æ§"""

    def __init__(self, cases_dir: str = "cases"):
        self.base_cases_dir = Path(cases_dir)
        self.results = {}
        self.test_cases = {}  # æ”¹ç‚ºå­—å…¸ä¾†å„²å­˜æ¡ˆä¾‹
        print("ğŸ“Š TCK å¢å¼·çµ±è¨ˆåˆ†æå™¨å•Ÿå‹• (ä¸€å°å¤šæ¨¡å¼)")
        print("=" * 60)

    def _analyze_code_quality(self, source_code: str) -> Dict[str, Any]:
        """ä½¿ç”¨ radon åˆ†æåŸå§‹ç¢¼çš„å“è³ªã€‚"""
        if not RADON_AVAILABLE:
            return {
                "éŒ¯èª¤": "radon å¥—ä»¶æœªå®‰è£",
                "åœˆè¤‡é›œåº¦": -1,
                "å¯ç¶­è­·æ€§æŒ‡æ•¸": -1,
                "ç¨‹å¼ç¢¼è¡Œæ•¸": -1,
            }

        # åªæœ‰åœ¨ radon å¯ç”¨æ™‚æ‰åŸ·è¡Œä»¥ä¸‹ç¨‹å¼ç¢¼
        try:
            # 1. åœˆè¤‡é›œåº¦
            # mypy æœƒåœ¨æ­¤è™•å ±éŒ¯ï¼Œå› ç‚ºå®ƒç„¡æ³•ç¢ºå®š ComplexityVisitor å·²è¢«å®šç¾©
            # ä½†å› ç‚ºæˆ‘å€‘æœ‰ RADON_AVAILABLE å®ˆè¡›ï¼Œæ‰€ä»¥åŸ·è¡Œæ™‚æ˜¯å®‰å…¨çš„
            visitor = ComplexityVisitor.from_code(source_code)  # type: ignore
            # å‡è¨­æ¯å€‹åŸå§‹ç¢¼ç‰‡æ®µåªæœ‰ä¸€å€‹ä¸»è¦å‡½å¼
            complexity = visitor.functions[0].complexity if visitor.functions else -1

            # 2. å¯ç¶­è­·æ€§æŒ‡æ•¸
            maintainability_index = mi_visit(source_code, multi=True)  # type: ignore

            # 3. åŸå§‹ç¢¼æŒ‡æ¨™ (è¡Œæ•¸)
            raw_metrics = analyze_raw(source_code)  # type: ignore
            sloc = raw_metrics.sloc

            # 4. ä¾è³´å¥—ä»¶æ•¸é‡åˆ†æ
            import re
            import_lines = re.findall(r'^\s*(?:import|from)\s+(\w+)', source_code, re.MULTILINE)
            # éæ¿¾æ¨™æº–åº«å’Œå¸¸è¦‹å…§å»ºæ¨¡çµ„
            standard_libs = {
                'os', 'sys', 're', 'math', 'time', 'datetime', 'json', 'collections',
                'itertools', 'functools', 'operator', 'pathlib', 'typing', 'inspect',
                'gc', 'platform', 'threading', 'multiprocessing', 'subprocess'
            }
            external_deps = [imp for imp in import_lines if imp not in standard_libs]
            dependency_count = len(set(external_deps))  # å»é‡

            # 5. æŠ€è¡“æ£§è¤‡é›œåº¦åˆ†æ
            tech_indicators = {
                'numpy': 2, 'numba': 3, 'pandas': 2, 'scipy': 2,
                'torch': 3, 'tensorflow': 3, 'jax': 3, 'cython': 3,
                'multiprocessing': 2, 'concurrent': 2, 'asyncio': 2
            }
            tech_complexity = 1  # é è¨­ç´” Python
            for dep in external_deps:
                if dep in tech_indicators:
                    tech_complexity = max(tech_complexity, tech_indicators[dep])

            return {
                "åœˆè¤‡é›œåº¦": complexity,
                "å¯ç¶­è­·æ€§æŒ‡æ•¸": maintainability_index,
                "ç¨‹å¼ç¢¼è¡Œæ•¸": sloc,
                "ä¾è³´å¥—ä»¶æ•¸é‡": dependency_count,
                "æŠ€è¡“æ£§è¤‡é›œåº¦": tech_complexity,
            }
        except Exception as e:
            return {
                "éŒ¯èª¤": str(e),
                "åœˆè¤‡é›œåº¦": -1,
                "å¯ç¶­è­·æ€§æŒ‡æ•¸": -1,
                "ç¨‹å¼ç¢¼è¡Œæ•¸": -1,
            }

    def discover_cases(self):
        """éè¿´åœ°å¾ cases ç›®éŒ„åŠå…¶å­ç›®éŒ„ï¼ˆå¦‚ microï¼‰ä¸­ç™¼ç¾æ¸¬è©¦æ¡ˆä¾‹ã€‚- TCK å„ªåŒ–ç‰ˆæœ¬"""
        print(f"ğŸ” æ­£åœ¨å¾ '{self.base_cases_dir}' åŠå…¶å­ç›®éŒ„æ¢ç´¢æ¸¬è©¦æ¡ˆä¾‹...")

        # å„ªåŒ–1: ä½¿ç”¨ç”Ÿæˆå™¨è¡¨é”å¼é€²è¡Œæ–‡ä»¶éæ¿¾ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
        valid_files = (
            path
            for path in self.base_cases_dir.rglob("*.py")
            if not path.name.startswith("__")
        )

        # å„ªåŒ–2: æ‰¹é‡è™•ç†æ¨¡çµ„è¼‰å…¥ï¼Œæ¸›å°‘é‡è¤‡æ“ä½œ
        for path in valid_files:
            case_name = path.stem

            # å„ªåŒ–3: ä½¿ç”¨ get() æ›¿ä»£å¯èƒ½çš„ç•°å¸¸è™•ç†
            try:
                spec = importlib.util.spec_from_file_location(case_name, path)
                if not (spec and spec.loader):
                    print(f"âš ï¸ ç„¡æ³•ç‚º {path} å»ºç«‹æ¨¡çµ„è¦ç¯„ã€‚")
                    continue

                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # å„ªåŒ–4: ä½¿ç”¨ getattr() èˆ‡é è¨­å€¼ï¼Œé¿å… AttributeError
                module_name = getattr(module, "name", case_name)
                self.test_cases[module_name] = module

            except Exception as e:
                print(f"âŒ è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹ '{case_name}' å¤±æ•—: {e}")

        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹ã€‚")

    def load_test_cases(self) -> bool:
        """å‹•æ…‹åœ°å¾ 'cases' ç›®éŒ„åŠå…¶æ‰€æœ‰å­ç›®éŒ„ä¸­è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹æª”æ¡ˆã€‚"""
        self.test_cases = {}
        if not self.base_cases_dir.is_dir():
            print(f"âŒ éŒ¯èª¤: æ¡ˆä¾‹ç›®éŒ„ '{self.base_cases_dir}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€å€‹ç›®éŒ„ã€‚")
            return False

        print(f"ğŸ” æ­£åœ¨å¾ '{self.base_cases_dir}' åŠå…¶å­ç›®éŒ„æ¢ç´¢æ¸¬è©¦æ¡ˆä¾‹...")

        # ä½¿ç”¨ rglob é€²è¡Œéè¿´æœç´¢
        for case_file in self.base_cases_dir.rglob("*.py"):
            if case_file.name.startswith("__"):
                continue

            try:
                module_name = case_file.stem
                spec = importlib.util.spec_from_file_location(module_name, case_file)
                if spec is None or spec.loader is None:
                    print(f"âš ï¸  è­¦å‘Š: ç„¡æ³•ç‚º '{case_file.name}' å‰µå»ºæ¨¡çµ„è¦æ ¼ã€‚")
                    continue

                test_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(test_module)

                # çµ±ä¸€è™•ç†æ¨¡çµ„å‹æ¡ˆä¾‹
                case_name = getattr(test_module, "name", module_name)
                description = getattr(test_module, "description", "ç„¡æè¿°")
                setup_data = getattr(test_module, "setup_data")
                unoptimized_func = getattr(test_module, "unoptimized_version")
                optimized_versions = getattr(test_module, "optimized_versions", {})

                # æª¢æŸ¥å¿…è¦å±¬æ€§æ˜¯å¦å­˜åœ¨
                if not all([case_name, description, setup_data, unoptimized_func]):
                    print(
                        f"âš ï¸  è­¦å‘Š: æ¡ˆä¾‹ '{case_file.name}' ç¼ºå°‘å¿…è¦çš„å±¬æ€§ (name, description, setup_data, unoptimized_version)ã€‚è·³éæ­¤æ¡ˆä¾‹ã€‚"
                    )
                    continue

                self.test_cases[case_name] = {
                    "name": case_name,
                    "description": description,
                    "setup_data": setup_data,
                    "unoptimized": unoptimized_func,
                    "optimized_versions": optimized_versions,
                    "module": test_module,
                }
            except Exception as e:
                print(f"âš ï¸  è­¦å‘Š: è¼‰å…¥æ¡ˆä¾‹ '{case_file.name}' å¤±æ•—: {e}")

        if not self.test_cases:
            print(f"âŒ éŒ¯èª¤: åœ¨ '{self.base_cases_dir}' ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¸¬è©¦æ¡ˆä¾‹ã€‚")
            return False

        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹ã€‚")
        return True

    def run_test_case(self, case_name: str) -> Optional[Dict[str, Any]]:
        """
        åŸ·è¡Œå–®ä¸€æ¸¬è©¦æ¡ˆä¾‹ï¼ˆä¸€å°å¤šç‰ˆæœ¬ï¼‰ï¼Œä¸¦æ”¶é›†æ‰€æœ‰ç‰ˆæœ¬çš„å®Œæ•´çµ±è¨ˆæ•¸æ“šã€‚
        """
        # å¯¦ç¾å¤§å°å¯«ä¸æ•æ„Ÿçš„æŸ¥æ‰¾
        target_key = next(
            (k for k in self.test_cases if k.lower() == case_name.lower()), None
        )

        if not target_key:
            print(f"âŒ æ‰¾ä¸åˆ°æ¸¬è©¦æ¡ˆä¾‹: {case_name}")
            self.list_tests()
            return None

        case_data = self.test_cases[target_key]

        print(f"\nğŸ” æ¸¬è©¦æ¡ˆä¾‹: {case_data['name']}")
        print(f"ğŸ“ èªªæ˜: {case_data['description']}")
        print("-" * 50)

        # æº–å‚™æ¸¬è©¦è³‡æ–™
        test_data = case_data["setup_data"]()

        # 1. åŸ·è¡Œæœªå„ªåŒ–ç‰ˆæœ¬ä½œç‚ºåŸºæº– (é‡è¤‡æ¸¬è©¦ä»¥ç²å–çµ±è¨ˆå¯é æ€§)
        print("âŒ æ¸¬è©¦åŸºæº–ç‰ˆæœ¬ (unoptimized)...")
        unoptimized_func = case_data["unoptimized"]
        baseline_results_list = []
        for i in range(3):  # é‡è¤‡ 3 æ¬¡
            result = self.measure_comprehensive_performance(
                unoptimized_func, *test_data
            )
            if result["æˆåŠŸ"]:
                baseline_results_list.append(result)
            if len(baseline_results_list) >= 2:  # è‡³å°‘ 2 æ¬¡æˆåŠŸ
                break

        if not baseline_results_list:
            print("âŒ åŸºæº–ç‰ˆæœ¬åŸ·è¡Œå¤±æ•—")
            return None

        # è¨ˆç®—çµ±è¨ˆï¼šä¸­ä½æ•¸å’Œ IQR
        import statistics
        times = [r["ç´”åŸ·è¡Œæ™‚é–“_ç§’"] for r in baseline_results_list]
        baseline_median_time = statistics.median(times)
        if len(times) > 1:
            baseline_iqr = statistics.quantiles(times, n=4)[2] - statistics.quantiles(times, n=4)[0]
        else:
            baseline_iqr = 0

        # ä½¿ç”¨ä¸­ä½æ•¸ä½œç‚ºåŸºæº–
        baseline_results = baseline_results_list[0].copy()
        baseline_results["ç´”åŸ·è¡Œæ™‚é–“_ç§’"] = baseline_median_time
        baseline_results["IQR_ç§’"] = baseline_iqr

        unoptimized_source = inspect.getsource(unoptimized_func)
        unoptimized_quality = self._analyze_code_quality(unoptimized_source)

        all_versions_results = {
            "unoptimized": {
                "metrics": baseline_results,
                "source": unoptimized_source,
                "quality": unoptimized_quality,
            }
        }

        # 2. éæ­·ä¸¦æ¸¬è©¦æ‰€æœ‰å„ªåŒ–ç‰ˆæœ¬
        optimized_versions = case_data.get("optimized_versions", {})
        if not optimized_versions:
            print("âš ï¸  è­¦å‘Š: è©²æ¡ˆä¾‹æ²’æœ‰æä¾›å„ªåŒ–ç‰ˆæœ¬ã€‚")
            # å³ä½¿æ²’æœ‰å„ªåŒ–ç‰ˆæœ¬ï¼Œä¹Ÿå›å‚³åŸºæº–æ¸¬è©¦çš„çµæœ
            return {
                "æ¸¬è©¦åç¨±": case_data["name"],
                "æ¸¬è©¦æè¿°": case_data["description"],
                "ç‰ˆæœ¬æ¯”è¼ƒçµæœ": all_versions_results,
                "æˆåŠŸ": True,
            }

        for version_name, optimized_func in optimized_versions.items():
            print(f"âœ… æ¸¬è©¦å„ªåŒ–ç‰ˆæœ¬: {version_name}...")

            # é‡è¤‡æ¸¬è©¦å„ªåŒ–ç‰ˆæœ¬
            optimized_results_list = []
            for i in range(3):
                result = self.measure_comprehensive_performance(
                    optimized_func, *test_data
                )
                if result["æˆåŠŸ"]:
                    optimized_results_list.append(result)
                if len(optimized_results_list) >= 2:
                    break

            if not optimized_results_list:
                print(f"âš ï¸  å„ªåŒ–ç‰ˆæœ¬ {version_name} åŸ·è¡Œå¤±æ•—")
                continue

            # è¨ˆç®—çµ±è¨ˆ
            times = [r["ç´”åŸ·è¡Œæ™‚é–“_ç§’"] for r in optimized_results_list]
            optimized_median_time = statistics.median(times)
            if len(times) > 1:
                optimized_iqr = statistics.quantiles(times, n=4)[2] - statistics.quantiles(times, n=4)[0]
            else:
                optimized_iqr = 0

            optimized_results = optimized_results_list[0].copy()
            optimized_results["ç´”åŸ·è¡Œæ™‚é–“_ç§’"] = optimized_median_time
            optimized_results["IQR_ç§’"] = optimized_iqr

            # è¨ˆç®—èˆ‡åŸºæº–çš„æ¯”è¼ƒæŒ‡æ¨™
            comparison_metrics = self._compare_metrics(
                baseline_results, optimized_results
            )

            # åˆ†æç¨‹å¼ç¢¼å“è³ª
            optimized_source = inspect.getsource(optimized_func)
            optimized_quality = self._analyze_code_quality(optimized_source)

            performance_scores = self._calculate_performance_score(
                comparison_metrics, optimized_quality, unoptimized_quality
            )
            total_score = performance_scores["ç¸½é«”æ•ˆèƒ½è©•åˆ†"]
            practicality_score = performance_scores.get("å¯¦ç”¨æ€§è©•åˆ†", 0)
            grade = self._get_performance_grade(total_score, practicality_score)

            all_versions_results[version_name] = {
                "metrics": optimized_results,
                "comparison": comparison_metrics,
                "scores": performance_scores,
                "grade": grade,
                "source": optimized_source,
                "quality": optimized_quality,
            }

            # é¡¯ç¤ºå³æ™‚çµæœ
            print(f"  â±ï¸  åŸ·è¡Œæ™‚é–“æ”¹å–„: {comparison_metrics['æ™‚é–“æ”¹å–„å€æ•¸']:.1f} å€")
            print(f"  ğŸ† è©•åˆ†: {total_score:.1f}/100 - {grade}")

        # 3. çµ„åˆæœ€çµ‚å ±å‘Šæ•¸æ“š
        final_report = {
            "æ¸¬è©¦åç¨±": case_data["name"],
            "æ¸¬è©¦æè¿°": case_data["description"],
            "ç‰ˆæœ¬æ¯”è¼ƒçµæœ": all_versions_results,
            "æˆåŠŸ": True,
        }

        # æ¸…ç†è³‡æº
        if hasattr(case_data["module"], "cleanup_data"):
            case_data["module"].cleanup_data(*test_data)

        return final_report

    def _get_comprehensive_system_stats(self) -> Dict[str, Any]:
        """å–å¾—å®Œæ•´ç³»çµ±çµ±è¨ˆè³‡è¨Š - TCK å„ªåŒ–ç‰ˆæœ¬"""
        # å„ªåŒ–1: ä½¿ç”¨å­—å…¸æ¨å°å¼åˆå§‹åŒ–æ•¸å€¼çµ±è¨ˆ (æ¸›å°‘é‡è¤‡éµå€¼å°å‰µå»º)
        numeric_keys = [
            "è¨˜æ†¶é«”_MB",
            "è™›æ“¬è¨˜æ†¶é«”_MB",
            "è¨˜æ†¶é«”ä½¿ç”¨ç‡_%",
            "è®€å–æ¬¡æ•¸",
            "å¯«å…¥æ¬¡æ•¸",
            "è®€å–ä½å…ƒçµ„",
            "å¯«å…¥ä½å…ƒçµ„",
            "CPUä½¿ç”¨ç‡_%",
            "ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨ç‡_%",
            "ç³»çµ±å¯ç”¨è¨˜æ†¶é«”_GB",
            "ç£ç¢Ÿä½¿ç”¨ç‡_%",
            "ç¶²è·¯å‚³é€_KB",
            "ç¶²è·¯æ¥æ”¶_KB",
            "GPUæ•¸é‡",
            "GPUä½¿ç”¨ç‡_%",
            "GPUè¨˜æ†¶é«”ä½¿ç”¨_MB",
            "GPUæº«åº¦_C",
            "CPUæ ¸å¿ƒæ•¸",
            "åŸ·è¡Œç·’æ•¸",
        ]
        # å‰µå»ºæ··åˆé¡å‹å­—å…¸
        stats: Dict[str, Any] = {key: 0.0 for key in numeric_keys}
        stats["ç³»çµ±å¹³å°"] = "Unknown"

        # å„ªåŒ–2: åˆä½µç³»çµ±è³‡è¨Šæ”¶é›†ï¼Œæ¸›å°‘æ¢ä»¶æª¢æŸ¥
        if SYSTEM_INFO_AVAILABLE:
            stats["ç³»çµ±å¹³å°"] = platform.system()
            stats["CPUæ ¸å¿ƒæ•¸"] = float(os.cpu_count() or 0)
            stats["åŸ·è¡Œç·’æ•¸"] = float(threading.active_count())

        # å„ªåŒ–3: ä½¿ç”¨ getattr() é¿å…ç•°å¸¸è™•ç†ï¼Œåˆä½µå¤šå€‹ update èª¿ç”¨
        if PSUTIL_AVAILABLE and psutil:
            try:
                process = psutil.Process()
                memory = process.memory_info()
                io_stats = process.io_counters()

                # åˆä½µç¬¬ä¸€çµ„çµ±è¨ˆæ•¸æ“š
                process_stats = {
                    "è¨˜æ†¶é«”_MB": memory.rss / 1024 / 1024,
                    "è™›æ“¬è¨˜æ†¶é«”_MB": memory.vms / 1024 / 1024,
                    "è¨˜æ†¶é«”ä½¿ç”¨ç‡_%": process.memory_percent(),
                    "è®€å–æ¬¡æ•¸": float(io_stats.read_count),
                    "å¯«å…¥æ¬¡æ•¸": float(io_stats.write_count),
                    "è®€å–ä½å…ƒçµ„": float(io_stats.read_bytes),
                    "å¯«å…¥ä½å…ƒçµ„": float(io_stats.write_bytes),
                }

                # åˆä½µç¬¬äºŒçµ„çµ±è¨ˆæ•¸æ“š
                memory_info = psutil.virtual_memory()
                disk_info = psutil.disk_usage("/")
                net_info = psutil.net_io_counters()

                system_stats = {
                    "CPUä½¿ç”¨ç‡_%": psutil.cpu_percent(interval=0.1),
                    "ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨ç‡_%": memory_info.percent,
                    "ç³»çµ±å¯ç”¨è¨˜æ†¶é«”_GB": memory_info.available / (1024**3),
                    "ç£ç¢Ÿä½¿ç”¨ç‡_%": (disk_info.used / disk_info.total) * 100,
                    "ç¶²è·¯å‚³é€_KB": getattr(net_info, "bytes_sent", 0) / 1024,
                    "ç¶²è·¯æ¥æ”¶_KB": getattr(net_info, "bytes_recv", 0) / 1024,
                }

                # å–®æ¬¡æ›´æ–°æ›¿ä»£å¤šæ¬¡ update
                stats.update(process_stats)
                stats.update(system_stats)

            except Exception as e:
                print(f"âš ï¸  psutil ç›£æ§éŒ¯èª¤: {e}")

        # å„ªåŒ–4: GPU çµ±è¨ˆæ•¸æ“šæ”¶é›†å„ªåŒ–
        if GPU_AVAILABLE and GPUtil:
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    stats.update(
                        {
                            "GPUæ•¸é‡": float(len(gpus)),
                            "GPUä½¿ç”¨ç‡_%": gpu.load * 100,
                            "GPUè¨˜æ†¶é«”ä½¿ç”¨_MB": float(getattr(gpu, "memoryUsed", 0)),
                            "GPUæº«åº¦_C": float(getattr(gpu, "temperature", 0)),
                        }
                    )
            except Exception as e:
                print(f"âš ï¸  GPUtil ç›£æ§éŒ¯èª¤: {e}")

        return stats

    def measure_comprehensive_performance(
        self, func, *args, **kwargs
    ) -> Dict[str, Any]:
        """æ¸¬é‡å®Œæ•´ç³»çµ±æ•ˆèƒ½çµ±è¨ˆ"""
        gc.collect()
        time.sleep(0.1)

        startup_start = time.perf_counter()
        start_time = time.perf_counter()
        start_cpu = time.process_time()
        start_stats = self._get_comprehensive_system_stats()
        startup_time = time.perf_counter() - startup_start

        result, success, error_msg = None, False, None
        execution_start = time.perf_counter()
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            error_msg = str(e)
        finally:
            execution_end = time.perf_counter()

        end_cpu = time.process_time()
        end_stats = self._get_comprehensive_system_stats()

        return {
            "ç´”åŸ·è¡Œæ™‚é–“_ç§’": execution_end - execution_start,
            "ç¸½åŸ·è¡Œæ™‚é–“_ç§’": time.perf_counter() - start_time,
            "CPUæ™‚é–“_ç§’": end_cpu - start_cpu,
            "å•Ÿå‹•æ™‚é–“_ç§’": startup_time,
            "åŸ·è¡Œå‰çµ±è¨ˆ": start_stats,
            "åŸ·è¡Œå¾Œçµ±è¨ˆ": end_stats,
            "çµæœ": result,
            "æˆåŠŸ": success,
            "éŒ¯èª¤è¨Šæ¯": error_msg,
        }

    def _compare_metrics(
        self, baseline: Dict[str, Any], optimized: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ¯”è¼ƒå…©å€‹æ•ˆèƒ½çµæœä¸¦è¨ˆç®—æ”¹å–„æŒ‡æ¨™ - TCK å„ªåŒ–ç‰ˆæœ¬"""
        # å„ªåŒ–1: ä½¿ç”¨ get() æ–¹æ³•é¿å…æ½›åœ¨çš„ KeyErrorï¼Œæä¾›é è¨­å€¼
        baseline_time = baseline.get("ç´”åŸ·è¡Œæ™‚é–“_ç§’", 1e-9)
        optimized_time = optimized.get("ç´”åŸ·è¡Œæ™‚é–“_ç§’", 1e-9)
        baseline_cpu = baseline.get("CPUæ™‚é–“_ç§’", 1e-9)
        optimized_cpu = optimized.get("CPUæ™‚é–“_ç§’", 1e-9)

        time_improvement = self._safe_divide(baseline_time, optimized_time)
        cpu_improvement = self._safe_divide(baseline_cpu, optimized_cpu)

        # åŠ å…¥çµ•å°æ™‚é–“æ”¹å–„æª¢æŸ¥ï¼šè‡³å°‘ç¯€çœ 0.01 ç§’æ‰æœ‰æ„ç¾©
        absolute_time_saving = baseline_time - optimized_time
        if absolute_time_saving < 0.01 and baseline_time > 0.1:  # å°æ–¼æ…¢å‡½æ•¸
            time_improvement *= 0.5  # æ‡²ç½°é‚Šéš›çµ•å°æ”¹å–„

        # å„ªåŒ–2: ä½¿ç”¨ get() å’ŒåµŒå¥—å­—å…¸å®‰å…¨è¨ªå•ï¼Œé¿å…é‡è¤‡å­—å…¸æŸ¥è©¢
        opt_after = optimized.get("åŸ·è¡Œå¾Œçµ±è¨ˆ", {})
        opt_before = optimized.get("åŸ·è¡Œå‰çµ±è¨ˆ", {})
        memory_change = opt_after.get("è¨˜æ†¶é«”_MB", 0.0) - opt_before.get(
            "è¨˜æ†¶é«”_MB", 0.0
        )

        # å„ªåŒ–3: æ‰¹é‡è™•ç† IO çµ±è¨ˆï¼Œæ¸›å°‘å­—å…¸æŸ¥è©¢æ¬¡æ•¸
        baseline_after = baseline.get("åŸ·è¡Œå¾Œçµ±è¨ˆ", {})
        baseline_before = baseline.get("åŸ·è¡Œå‰çµ±è¨ˆ", {})

        io_metrics = {
            "io_read_baseline": baseline_after.get("è®€å–æ¬¡æ•¸", 0)
            - baseline_before.get("è®€å–æ¬¡æ•¸", 0),
            "io_write_baseline": baseline_after.get("å¯«å…¥æ¬¡æ•¸", 0)
            - baseline_before.get("å¯«å…¥æ¬¡æ•¸", 0),
            "io_read_optimized": opt_after.get("è®€å–æ¬¡æ•¸", 0)
            - opt_before.get("è®€å–æ¬¡æ•¸", 0),
            "io_write_optimized": opt_after.get("å¯«å…¥æ¬¡æ•¸", 0)
            - opt_before.get("å¯«å…¥æ¬¡æ•¸", 0),
        }

        # å„ªåŒ–4: å®‰å…¨çš„çµæœæ¯”è¼ƒ
        correctness = self._verify_correctness(
            baseline.get("çµæœ"), optimized.get("çµæœ")
        )

        # å„ªåŒ–5: å–®æ¬¡å­—å…¸å‰µå»ºï¼Œé¿å…å¤šæ¬¡è³¦å€¼
        return {
            "æ™‚é–“æ”¹å–„å€æ•¸": time_improvement,
            "CPUæ”¹å–„å€æ•¸": cpu_improvement,
            "è¨˜æ†¶é«”è®ŠåŒ–_MB": memory_change,
            "IOçµ±è¨ˆ": {
                "åŸå§‹è®€å–æ¬¡æ•¸": io_metrics["io_read_baseline"],
                "å„ªåŒ–è®€å–æ¬¡æ•¸": io_metrics["io_read_optimized"],
                "åŸå§‹å¯«å…¥æ¬¡æ•¸": io_metrics["io_write_baseline"],
                "å„ªåŒ–å¯«å…¥æ¬¡æ•¸": io_metrics["io_write_optimized"],
            },
            "æ­£ç¢ºæ€§": correctness,
        }

    def _safe_divide(
        self, numerator: float, denominator: float, min_threshold: float = 1e-9
    ) -> float:
        """å®‰å…¨é™¤æ³•ï¼Œé¿å… infinity"""
        if abs(denominator) < min_threshold:
            return 9999.9
        return numerator / denominator

    def _calculate_performance_score(
        self, metrics: Dict[str, Any], quality_metrics: Dict[str, Any], baseline_quality: Optional[Dict[str, Any]] = None
    ) -> Dict[str, float]:
        """è¨ˆç®—æ•ˆèƒ½è©•åˆ† (0-100 åˆ†)ï¼ŒåŒ…å«ç¨‹å¼ç¢¼å“è³ª - ä½¿ç”¨TCKæœ€ä½³åŒ–æŠ€è¡“ï¼šæ˜ å°„è¡¨å’Œæ‰¹æ¬¡è¨ˆç®—"""

        # TCK å„ªåŒ–ï¼šé å®šç¾©è©•åˆ†è¨ˆç®—å‡½æ•¸ï¼Œé¿å…å¤šé‡æ¢ä»¶åˆ¤æ–·
        def calculate_time_score(improvement: float) -> float:
            # åŠ å…¥æœ€å°é¡¯è‘—æ”¹å–„é–€æª»ï¼šå°æ–¼1.5xçš„æ”¹å–„è¦–ç‚ºé‚Šéš›æ•ˆç›Šä¸è¶³
            if improvement < 1.5:
                return max(0.0, improvement * 20)  # é¡¯è‘—é™ä½åˆ†æ•¸
            if improvement >= 50:
                return 100.0
            elif improvement >= 10:
                return 80.0 + (improvement - 10) * 0.5
            elif improvement >= 2:
                return 60.0 + (improvement - 2) * 2.5
            return max(0.0, improvement * 30)

        def calculate_cpu_score(improvement: float) -> float:
            if improvement >= 999:
                return 100.0
            elif improvement >= 10:
                return 90.0
            elif improvement >= 5:
                return 80.0
            elif improvement >= 2:
                return 70.0
            return max(0.0, improvement * 35)

        def calculate_memory_score(change: float) -> float:
            if change <= 0:
                return 100.0
            elif change <= 10:
                return 90.0 - change * 2
            elif change <= 50:
                return 70.0 - (change - 10) * 1.5
            return max(10.0, 70.0 - change * 0.5)

        def calculate_io_score(reduction: int) -> float:
            if reduction >= 100:
                return 100.0
            elif reduction >= 50:
                return 90.0
            elif reduction >= 10:
                return 80.0
            elif reduction > 0:
                return 60.0 + reduction * 2
            return 50.0

        # TCK å„ªåŒ–ï¼šä½¿ç”¨å­—å…¸ get() æ–¹æ³•å’Œæ‰¹æ¬¡æå–æ•¸æ“š
        time_improvement = metrics.get("æ™‚é–“æ”¹å–„å€æ•¸", 1.0)
        cpu_improvement = metrics.get("CPUæ”¹å–„å€æ•¸", 1.0)
        memory_change = metrics.get("è¨˜æ†¶é«”è®ŠåŒ–_MB", 0)

        # è¨ˆç®— IO æ•ˆç‡ï¼ˆå®‰å…¨å­—å…¸å­˜å–ï¼‰
        io_stats = metrics.get("IOçµ±è¨ˆ", {})
        io_reduction = io_stats.get("åŸå§‹è®€å–æ¬¡æ•¸", 0) - io_stats.get("å„ªåŒ–è®€å–æ¬¡æ•¸", 0)

        # TCK å„ªåŒ–ï¼šæ‰¹æ¬¡è¨ˆç®—æ ¸å¿ƒè©•åˆ†ï¼Œé¿å…é€ä¸€è³¦å€¼
        scores = {
            "æ™‚é–“æ”¹å–„è©•åˆ†": calculate_time_score(time_improvement),
            "CPUæ•ˆç‡è©•åˆ†": calculate_cpu_score(cpu_improvement),
            "è¨˜æ†¶é«”æ•ˆç‡è©•åˆ†": calculate_memory_score(memory_change),
            "IOæ•ˆç‡è©•åˆ†": calculate_io_score(io_reduction),
        }

        # ç¨‹å¼ç¢¼å“è³ªè©•åˆ†è¨ˆç®— - ä½¿ç”¨ TCK å®‰å…¨å­—å…¸å­˜å–
        quality_score = 0.0
        practicality_score = 0.0
        if quality_metrics and "éŒ¯èª¤" not in quality_metrics:
            # TCK å„ªåŒ–ï¼šä½¿ç”¨æ˜ å°„è¡¨é¿å…å¤šé‡æ¢ä»¶åˆ¤æ–·
            complexity_thresholds = [(5, 100), (10, 80), (20, 60)]
            length_thresholds = [(15, 100), (30, 80), (60, 60)]

            mi_score = max(0, quality_metrics.get("å¯ç¶­è­·æ€§æŒ‡æ•¸", 0))
            cc = quality_metrics.get("åœˆè¤‡é›œåº¦", 25)
            sloc = quality_metrics.get("ç¨‹å¼ç¢¼è¡Œæ•¸", 100)

            # ä½¿ç”¨ next() å’Œç”Ÿæˆå™¨è¡¨é”å¼å¿«é€ŸæŸ¥æ‰¾è©•åˆ†
            cc_score = next(
                (
                    score
                    for threshold, score in complexity_thresholds
                    if cc <= threshold
                ),
                30,
            )
            sloc_score = next(
                (score for threshold, score in length_thresholds if sloc <= threshold),
                30,
            )

            # æ–°å¢å¯¦ç”¨æ€§æŒ‡æ¨™
            dependency_count = quality_metrics.get("ä¾è³´å¥—ä»¶æ•¸é‡", 0)
            tech_stack_complexity = quality_metrics.get("æŠ€è¡“æ£§è¤‡é›œåº¦", 1)
            complexity_ratio = 1.0
            if baseline_quality:
                baseline_sloc = baseline_quality.get("ç¨‹å¼ç¢¼è¡Œæ•¸", 1)
                optimized_sloc = quality_metrics.get("ç¨‹å¼ç¢¼è¡Œæ•¸", 1)
                complexity_ratio = optimized_sloc / baseline_sloc if baseline_sloc > 0 else 1.0

            # ä¾è³´æ•¸é‡è©•åˆ†ï¼šè¶Šå°‘è¶Šå¥½
            if dependency_count == 0:
                dep_score = 100
            elif dependency_count <= 2:
                dep_score = 80
            elif dependency_count <= 5:
                dep_score = 60
            else:
                dep_score = max(20, 60 - (dependency_count - 5) * 10)

            # æŠ€è¡“æ£§è¤‡é›œåº¦è©•åˆ†ï¼š1=ç´”Python, 2=NumPy, 3=Numbaç­‰
            if tech_stack_complexity == 1:
                tech_score = 100
            elif tech_stack_complexity == 2:
                tech_score = 80
            elif tech_stack_complexity == 3:
                tech_score = 60
            else:
                tech_score = max(20, 60 - (tech_stack_complexity - 3) * 20)

            # è¤‡é›œåº¦å€æ•¸è©•åˆ†ï¼šå„ªåŒ–è¡Œæ•¸/åŸå§‹è¡Œæ•¸ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½
            if complexity_ratio <= 1.2:
                ratio_score = 100
            elif complexity_ratio <= 2.0:
                ratio_score = 80
            elif complexity_ratio <= 5.0:
                ratio_score = 60
            else:
                ratio_score = max(20, 60 - (complexity_ratio - 5) * 5)

            # TCK å„ªåŒ–ï¼šé è¨ˆç®—æ¬Šé‡ï¼Œå–®æ¬¡è¨ˆç®—
            quality_score = mi_score * 0.4 + cc_score * 0.3 + sloc_score * 0.3
            # æŠ€è¡“æ£§å£½å‘½è©•åˆ†ï¼šè€ƒæ…®é•·æœŸç¶­è­·æˆæœ¬
            # Numba/Torch ç­‰è¤‡é›œæŠ€è¡“å¯èƒ½æœ‰ç›¸å®¹æ€§å•é¡Œï¼Œçµ¦ä½åˆ†
            tech_lifetime_score = 100
            if tech_stack_complexity >= 3:
                tech_lifetime_score = 60  # è¤‡é›œæŠ€è¡“é•·æœŸç¶­è­·æˆæœ¬é«˜
            elif tech_stack_complexity == 2:
                tech_lifetime_score = 80

            practicality_score = dep_score * 0.3 + tech_score * 0.3 + ratio_score * 0.2 + tech_lifetime_score * 0.2

        scores["ç¨‹å¼ç¢¼å“è³ªè©•åˆ†"] = quality_score
        scores["å¯¦ç”¨æ€§è©•åˆ†"] = practicality_score

        # TCK å„ªåŒ–ï¼šé å®šç¾©æ¬Šé‡å­—å…¸ï¼Œä½¿ç”¨ç”Ÿæˆå™¨è¡¨é”å¼è¨ˆç®—ç¸½åˆ†
        # èª¿æ•´æ¬Šé‡ï¼šæ¸›å°‘æ•ˆèƒ½æ¬Šé‡ï¼Œå¢åŠ å“è³ªèˆ‡å¯¦ç”¨æ€§æ¬Šé‡ä»¥å¹³è¡¡å¯¦ç”¨æ€§
        weights = {
            "æ™‚é–“æ”¹å–„è©•åˆ†": 0.25,  # é€²ä¸€æ­¥é™è‡³ 0.25
            "CPUæ•ˆç‡è©•åˆ†": 0.15,  # é™è‡³ 0.15
            "è¨˜æ†¶é«”æ•ˆç‡è©•åˆ†": 0.10,  # é™è‡³ 0.10
            "IOæ•ˆç‡è©•åˆ†": 0.05,
            "ç¨‹å¼ç¢¼å“è³ªè©•åˆ†": 0.20,  # é™è‡³ 0.20
            "å¯¦ç”¨æ€§è©•åˆ†": 0.25,  # æ–°å¢å¯¦ç”¨æ€§è©•åˆ†
        }
        scores["ç¸½é«”æ•ˆèƒ½è©•åˆ†"] = sum(
            scores[key] * weight for key, weight in weights.items()
        )

        # æ­£ç¢ºæ€§æ‡²ç½°ï¼šå¦‚æœçµæœä¸æ­£ç¢ºï¼Œç¸½åˆ†æ¸›åŠ
        correctness = metrics.get("æ­£ç¢ºæ€§", True)
        if not correctness:
            scores["ç¸½é«”æ•ˆèƒ½è©•åˆ†"] *= 0.5
            scores["æ­£ç¢ºæ€§æ‡²ç½°"] = True

        return scores

    def _get_performance_grade(self, score: float, practicality_score: float = 0) -> str:
        """æ ¹æ“šè©•åˆ†ç²å¾—ç­‰ç´š - è€ƒæ…®å¯¦ç”¨æ€§æ¬Šè¡¡"""
        # èª¿æ•´é–€æª»ï¼šæé«˜ Aç´šè¦æ±‚ï¼ŒåŠ å…¥å¯¦ç”¨æ€§æ‡²ç½°
        if practicality_score < 60:  # å¯¦ç”¨æ€§å·®ï¼Œé™ç´š
            grade_thresholds = (
                (98, "A+ (å“è¶Š)"),
                (90, "A (å„ªç§€)"),
                (80, "B+ (è‰¯å¥½)"),
                (70, "B (ä¸­ç­‰)"),
                (60, "C+ (åˆæ ¼)"),
                (50, "C (å‹‰å¼·é€šé)"),
            )
        else:  # å¯¦ç”¨æ€§å¥½ï¼Œæ­£å¸¸é–€æª»
            grade_thresholds = (
                (95, "A+ (å“è¶Š)"),
                (85, "A (å„ªç§€)"),
                (75, "B+ (è‰¯å¥½)"),
                (65, "B (ä¸­ç­‰)"),
                (55, "C+ (åˆæ ¼)"),
                (45, "C (å‹‰å¼·é€šé)"),
            )

        return next(
            (grade for threshold, grade in grade_thresholds if score >= threshold),
            "D (éœ€è¦æ”¹å–„)",
        )

    def _deep_compare(self, obj1: Any, obj2: Any, precision: int = 5) -> bool:
        """
        éè¿´æ¯”è¼ƒå…©å€‹ç‰©ä»¶ï¼Œç‰¹åˆ¥è™•ç†æµ®é»æ•¸å’Œ NumPy å‹åˆ¥çš„ç²¾åº¦å•é¡Œã€‚
        ä½¿ç”¨TCKæœ€ä½³åŒ–æŠ€è¡“ï¼šé¡å‹æª¢æŸ¥æœ€ä½³åŒ–å’Œæ—©æœŸè¿”å›
        """
        # TCK å„ªåŒ–ï¼šä½¿ç”¨ hasattr() æ‰¹æ¬¡æª¢æŸ¥ï¼Œé¿å…é‡è¤‡èª¿ç”¨
        if hasattr(obj1, "item"):
            obj1 = obj1.item()
        if hasattr(obj2, "item"):
            obj2 = obj2.item()

        # TCK å„ªåŒ–ï¼šé¡å‹çµ±ä¸€æª¢æŸ¥ï¼Œæ¸›å°‘é‡è¤‡çš„ type() èª¿ç”¨
        obj1_type, obj2_type = type(obj1), type(obj2)

        # å‹åˆ¥ä¸åŒæ™‚çš„æ•¸å­—æ¯”è¼ƒå„ªåŒ–
        if obj1_type is not obj2_type:
            # TCK å„ªåŒ–ï¼šä½¿ç”¨å…ƒçµ„åŒ…å«æª¢æŸ¥ï¼Œé¿å…å¤šæ¬¡ isinstance èª¿ç”¨
            if obj1_type in (int, float) and obj2_type in (int, float):
                return math.isclose(
                    obj1, obj2, rel_tol=1e-9, abs_tol=10 ** (-precision)
                )
            return False

        # TCK å„ªåŒ–ï¼šä½¿ç”¨æ—©æœŸè¿”å›å’Œé è¨ˆç®—é–¾å€¼
        abs_tolerance = 10 ** (-precision)

        # å­—å…¸æ¯”è¼ƒå„ªåŒ–
        if obj1_type is dict:
            # TCK å„ªåŒ–ï¼šå…ˆæª¢æŸ¥éµé›†åˆï¼Œé¿å…ä¸å¿…è¦çš„éè¿´
            if obj1.keys() != obj2.keys():
                return False
            # ä½¿ç”¨ all() å’Œç”Ÿæˆå™¨è¡¨é”å¼
            return all(self._deep_compare(obj1[k], obj2[k], precision) for k in obj1)

        # åºåˆ—æ¯”è¼ƒå„ªåŒ–ï¼ˆlist, tupleï¼‰
        if obj1_type in (list, tuple):
            # TCK å„ªåŒ–ï¼šé•·åº¦æª¢æŸ¥å„ªå…ˆ
            if len(obj1) != len(obj2):
                return False
            # ä½¿ç”¨ zip() å’Œ all() æ‰¹æ¬¡æ¯”è¼ƒ
            return all(
                self._deep_compare(i1, i2, precision) for i1, i2 in zip(obj1, obj2)
            )

        # æµ®é»æ•¸æ¯”è¼ƒ
        if obj1_type is float:
            return math.isclose(obj1, obj2, rel_tol=1e-9, abs_tol=abs_tolerance)

        # TCK å„ªåŒ–ï¼šå…¶ä»–å‹åˆ¥ç›´æ¥ç›¸ç­‰æ¯”è¼ƒ
        return obj1 == obj2

    def _verify_correctness(self, original_result, optimized_result) -> bool:
        """
        ä½¿ç”¨æ·±åº¦æ¯”è¼ƒé©—è­‰çµæœçš„æ­£ç¢ºæ€§ï¼Œä»¥è™•ç†æµ®é»æ•¸ç²¾åº¦å•é¡Œã€‚
        ä½¿ç”¨TCKæœ€ä½³åŒ–æŠ€è¡“ï¼šæ—©æœŸè¿”å›å’Œç•°å¸¸è™•ç†æœ€ä½³åŒ–
        """
        # TCK å„ªåŒ–ï¼šæ—©æœŸè¿”å›ï¼Œé¿å…ä¸å¿…è¦çš„è¨ˆç®—
        if original_result is None and optimized_result is None:
            return True
        if original_result is None or optimized_result is None:
            return False

        try:
            return self._deep_compare(original_result, optimized_result)
        except Exception as e:
            # TCK å„ªåŒ–ï¼šä½¿ç”¨ f-string ä»£æ›¿å­—ä¸²æ ¼å¼åŒ–
            print(f"âš ï¸  æ­£ç¢ºæ€§é©—è­‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def _print_baseline_report(
        self, result: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """æ‰“å°åŸºæº–ç‰ˆæœ¬å ±å‘Š - TCKå„ªåŒ–ï¼šå‡½æ•¸æ‹†åˆ†é™ä½è¤‡é›œåº¦"""
        baseline_data = result["ç‰ˆæœ¬æ¯”è¼ƒçµæœ"].get("unoptimized")
        if not baseline_data:
            print("âŒ å ±å‘ŠéŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åŸºæº–ç‰ˆæœ¬ (unoptimized) çš„æ•¸æ“šã€‚")
            return None

        baseline_metrics = baseline_data["metrics"]
        baseline_quality = baseline_data["quality"]
        print("### åŸºæº–ç‰ˆæœ¬ (unoptimized)")
        print(f"  - åŸ·è¡Œæ™‚é–“: {baseline_metrics['ç´”åŸ·è¡Œæ™‚é–“_ç§’']:.6f} ç§’")
        print(f"  - CPU æ™‚é–“: {baseline_metrics['CPUæ™‚é–“_ç§’']:.6f} ç§’")
        print(
            f"  - è¨˜æ†¶é«”ä½¿ç”¨ (é–‹å§‹/çµæŸ): {baseline_metrics['åŸ·è¡Œå‰çµ±è¨ˆ']['è¨˜æ†¶é«”_MB']:.2f} MB / {baseline_metrics['åŸ·è¡Œå¾Œçµ±è¨ˆ']['è¨˜æ†¶é«”_MB']:.2f} MB"
        )
        if "éŒ¯èª¤" not in baseline_quality:
            print(
                f"  - ç¨‹å¼ç¢¼å“è³ª: CC={baseline_quality['åœˆè¤‡é›œåº¦']}, MI={baseline_quality['å¯ç¶­è­·æ€§æŒ‡æ•¸']:.1f}, Lines={baseline_quality['ç¨‹å¼ç¢¼è¡Œæ•¸']}"
            )
        print("-" * 30)
        return baseline_data

    def _print_optimized_versions_report(self, result: Dict[str, Any]):
        """æ‰“å°å„ªåŒ–ç‰ˆæœ¬å ±å‘Š - TCKå„ªåŒ–ï¼šå‡½æ•¸æ‹†åˆ†é™ä½è¤‡é›œåº¦"""
        optimized_versions_data = {
            k: v for k, v in result["ç‰ˆæœ¬æ¯”è¼ƒçµæœ"].items() if k != "unoptimized"
        }

        if not optimized_versions_data:
            print("### è©²æ¡ˆä¾‹æ²’æœ‰æä¾›å¯æ¯”è¼ƒçš„å„ªåŒ–ç‰ˆæœ¬ã€‚")
            print("=" * 60)
            return

        # TCK å„ªåŒ–ï¼šä½¿ç”¨ sorted() è€Œéæ‰‹å‹•æ’åº
        sorted_versions = sorted(
            optimized_versions_data.items(),
            key=lambda item: item[1]["scores"]["ç¸½é«”æ•ˆèƒ½è©•åˆ†"],
            reverse=True,
        )

        print("### å„ªåŒ–ç‰ˆæœ¬æ•ˆèƒ½æ’å")
        grade_emoji_map = {"A+": "ğŸ¥‡", "A": "ğŸ¥ˆ", "B+": "ğŸ¥‰"}

        for i, (version_name, version_data) in enumerate(sorted_versions, 1):
            comp = version_data["comparison"]
            scores = version_data["scores"]
            quality = version_data["quality"]
            grade_emoji = grade_emoji_map.get(version_data["grade"].split(" ")[0], "ğŸ“Š")

            print(
                f"{grade_emoji} {i}. {version_name} (è©•åˆ†: {scores['ç¸½é«”æ•ˆèƒ½è©•åˆ†']:.1f}/100 - {version_data['grade']})"
            )
            print(f"  - æ­£ç¢ºæ€§: {'âœ… æ­£ç¢º' if comp['æ­£ç¢ºæ€§'] else 'âŒ éŒ¯èª¤'}")
            print(f"  - æ™‚é–“æ”¹å–„: {comp['æ™‚é–“æ”¹å–„å€æ•¸']:.1f} å€ (vs unoptimized)")
            print(f"  - CPU æ”¹å–„: {comp['CPUæ”¹å–„å€æ•¸']:.1f} å€ (vs unoptimized)")
            print(f"  - è¨˜æ†¶é«”è®ŠåŒ–: {comp['è¨˜æ†¶é«”è®ŠåŒ–_MB']:+.2f} MB")
            if "éŒ¯èª¤" not in quality:
                print(
                    f"  - ç¨‹å¼ç¢¼å“è³ª: CC={quality['åœˆè¤‡é›œåº¦']}, MI={quality['å¯ç¶­è­·æ€§æŒ‡æ•¸']:.1f}, Lines={quality['ç¨‹å¼ç¢¼è¡Œæ•¸']} (å“è³ªè©•åˆ†: {scores['ç¨‹å¼ç¢¼å“è³ªè©•åˆ†']:.1f})"
                )
            print("-" * 30)

    def generate_detailed_report(self, result: Dict[str, Any]):
        """ç‚ºä¸€å°å¤šæ¯”è¼ƒçµæœç”Ÿæˆè©³ç´°æ¸¬è©¦å ±å‘Š - TCKå„ªåŒ–ï¼šå‡½æ•¸æ‹†åˆ†é™ä½è¤‡é›œåº¦"""
        if not result or not result.get("æˆåŠŸ"):
            print("âŒ ç„¡æ³•ç”Ÿæˆå ±å‘Šï¼šæ¸¬è©¦åŸ·è¡Œå¤±æ•—æˆ–æ²’æœ‰çµæœã€‚")
            return

        print("\n" + "=" * 60)
        print(f"ğŸ“Š {result['æ¸¬è©¦åç¨±']} è©³ç´°æ•ˆèƒ½å ±å‘Š")
        print("=" * 60)

        # TCK å„ªåŒ–ï¼šå‡½æ•¸æ‹†åˆ†ï¼Œé™ä½è¤‡é›œåº¦
        baseline_data = self._print_baseline_report(result)
        if not baseline_data:
            return

        self._print_optimized_versions_report(result)

        # TCK å„ªåŒ–ï¼šå°‡å ±å‘Šä¿å­˜æ‹†åˆ†ç‚ºç¨ç«‹å‡½æ•¸
        self._save_detailed_report(result)

    def _save_detailed_report(self, result: Dict[str, Any]):
        """ä¿å­˜è©³ç´°å ±å‘Šåˆ° JSON æ–‡ä»¶ - TCKå„ªåŒ–ï¼šå‡½æ•¸æ‹†åˆ†é™ä½è¤‡é›œåº¦"""
        report_dir = Path("test_reports")
        report_dir.mkdir(exist_ok=True)
        timestamp = int(time.time())
        report_file = report_dir / f"TCKå ±å‘Š_{result['æ¸¬è©¦åç¨±']}_{timestamp}.json"

        # TCK å„ªåŒ–ï¼šä½¿ç”¨å­—å…¸æ¨å°ç°¡åŒ–æ•¸æ“šæ¸…ç†
        simplified_result = result.copy()
        for version, data in simplified_result["ç‰ˆæœ¬æ¯”è¼ƒçµæœ"].items():
            if "metrics" in data:
                # ç§»é™¤é¾å¤§çš„åŸ·è¡Œçµæœæ•¸æ“š
                for key in ["çµæœ", "åŸ·è¡Œå‰çµ±è¨ˆ", "åŸ·è¡Œå¾Œçµ±è¨ˆ"]:
                    data["metrics"].pop(key, None)

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(simplified_result, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ¯ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_file}")

    def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹ä¸¦ç”¢ç”Ÿç¶œåˆå ±å‘Š"""
        all_results = []

        for i, name in enumerate(self.test_cases.keys(), 1):
            print(f"\nğŸ”¬ åŸ·è¡Œæ¸¬è©¦ {i}/{len(self.test_cases)}: {name}")
            print("-" * 50)

            result = self.run_test_case(name)
            if result:
                all_results.append(result)
                # ç‚ºæ¯å€‹æ¸¬è©¦æ¡ˆä¾‹ç”Ÿæˆè©³ç´°å ±å‘Š
                self.generate_detailed_report(result)

        self.generate_summary_report(all_results)

    def run_specific_test(self, test_name: str):
        """åŸ·è¡ŒæŒ‡å®šçš„æ¸¬è©¦æ¡ˆä¾‹ - ä½¿ç”¨TCKæœ€ä½³åŒ–æŠ€è¡“ï¼šå­—å…¸æŸ¥æ‰¾æœ€ä½³åŒ–"""

        # TCK å„ªåŒ–ï¼šä½¿ç”¨å­—å…¸æ¨å°å’Œ next() é€²è¡Œé«˜æ•ˆæŸ¥æ‰¾
        target_name = next(
            (k for k in self.test_cases if k.upper() == test_name.upper()), None
        )

        if target_name is None:
            print(f"âŒ æ‰¾ä¸åˆ°æ¸¬è©¦æ¡ˆä¾‹: {test_name}")
            self.list_tests()
            return

        print(f"ğŸ¯ åŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹: {target_name}")
        print("=" * 60)

        result = self.run_test_case(target_name)
        if result:
            self.generate_detailed_report(result)

    def generate_summary_report(self, results: List[Dict[str, Any]]):
        """ç”¢ç”Ÿç¶œåˆçµ±è¨ˆå ±å‘Š - ä½¿ç”¨TCKæœ€ä½³åŒ–æŠ€è¡“ï¼šåˆ—è¡¨æ¨å°å’Œé è¨ˆç®—"""
        if not results:
            print("ğŸ“Š æ²’æœ‰æœ‰æ•ˆçš„æ¸¬è©¦çµæœå¯ä¾›ç”Ÿæˆå ±å‘Šã€‚")
            return

        print("\n" + "=" * 60)
        print("ğŸ“Š TCK ç¶œåˆæ•ˆèƒ½åˆ†æå ±å‘Š")
        print("=" * 60)

        # TCK å„ªåŒ–ï¼šä½¿ç”¨åˆ—è¡¨æ¨å°å’Œç”Ÿæˆå™¨è¡¨é”å¼ï¼Œé¿å…åµŒå¥—å¾ªç’°
        def extract_best_version(result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            """æå–æ¯å€‹æ¡ˆä¾‹çš„æœ€ä½³ç‰ˆæœ¬"""
            # ä½¿ç”¨å­—å…¸æ¨å°éæ¿¾å„ªåŒ–ç‰ˆæœ¬
            optimized_versions = {
                k: v for k, v in result["ç‰ˆæœ¬æ¯”è¼ƒçµæœ"].items() if k != "unoptimized"
            }

            if not optimized_versions:
                return None

            # TCK å„ªåŒ–ï¼šä½¿ç”¨ max() è€Œéæ‰‹å‹•è¿­ä»£æ‰¾æœ€å¤§å€¼
            best_version_name, best_version_data = max(
                optimized_versions.items(),
                key=lambda item: item[1]["scores"]["ç¸½é«”æ•ˆèƒ½è©•åˆ†"],
            )

            return {
                "case_name": result["æ¸¬è©¦åç¨±"],
                "version_name": best_version_name,
                "score": best_version_data["scores"]["ç¸½é«”æ•ˆèƒ½è©•åˆ†"],
                "grade": best_version_data["grade"],
                "improvement": best_version_data["comparison"]["æ™‚é–“æ”¹å–„å€æ•¸"],
            }

        # TCK å„ªåŒ–ï¼šä½¿ç”¨åˆ—è¡¨æ¨å°å’Œ filter() æ‰¹æ¬¡è™•ç†
        ranked_list = sorted(
            filter(None, (extract_best_version(result) for result in results)),
            key=lambda x: x["score"],
            reverse=True,
        )

        print("ğŸ† æ¸¬è©¦æ¡ˆä¾‹æ•ˆèƒ½æ’è¡Œæ¦œ (æœ€ä½³ç‰ˆæœ¬):")
        print("=" * 50)

        # TCK å„ªåŒ–ï¼šé å®šç¾©ç­‰ç´šæ˜ å°„è¡¨ï¼Œé¿å…é‡è¤‡å­—å…¸æŸ¥è©¢
        grade_emoji_map = {"A+": "ğŸ¥‡", "A": "ğŸ¥ˆ", "B+": "ğŸ¥‰"}

        for i, item in enumerate(ranked_list, 1):
            grade_prefix = item["grade"].split(" ")[0]
            grade_emoji = grade_emoji_map.get(grade_prefix, "ğŸ“Š")
            print(
                f"{grade_emoji} {i:2d}. {item['case_name']:<25} "
                f"è©•åˆ†: {item['score']:5.1f}/100 ({item['grade']}) "
                f"æå‡: {item['improvement']:6.1f}x"
            )

        print("\nğŸ“„ å®Œæ•´å ±å‘Šå·²ä¿å­˜è‡³å„å€‹è©³ç´°JSONæª”æ¡ˆ")
        print("=" * 60)

    def list_tests(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¸¬è©¦æ¡ˆä¾‹"""
        print("ğŸ“‹ å¯ç”¨çš„æ¸¬è©¦æ¡ˆä¾‹:")
        for case_name, case_data in self.test_cases.items():
            print(f" - {case_name}: {case_data['description']}")


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description="TCK å¢å¼·çµ±è¨ˆåˆ†æå™¨ - å®Œæ•´çš„æ•ˆèƒ½èˆ‡è³‡æºç›£æ§ (ä¸€å°å¤šæ¨¡å¼)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  python tck_enhanced_analyzer.py                # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
  python tck_enhanced_analyzer.py --list         # åˆ—å‡ºæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹
  python tck_enhanced_analyzer.py --test LIST_LOOKUP # åŸ·è¡Œç‰¹å®šæ¸¬è©¦
        """,
    )

    parser.add_argument(
        "--list", "-l", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¸¬è©¦æ¡ˆä¾‹"
    )
    parser.add_argument("--test", "-t", type=str, help="åŸ·è¡ŒæŒ‡å®šçš„æ¸¬è©¦æ¡ˆä¾‹")
    parser.add_argument("--cases-dir", "-d", default="cases", help="æ¸¬è©¦æ¡ˆä¾‹ç›®éŒ„è·¯å¾‘")

    args = parser.parse_args()

    # å‰µå»ºåˆ†æå™¨
    analyzer = TCKEnhancedAnalyzer(args.cases_dir)

    if not analyzer.load_test_cases():
        return

    # åŸ·è¡Œå‘½ä»¤
    if args.list:
        analyzer.list_tests()
    elif args.test:
        if args.test.upper() == "ALL":
            analyzer.run_all_tests()
        else:
            analyzer.run_specific_test(args.test)
    else:
        # å¦‚æœæ²’æœ‰æŒ‡å®šåƒæ•¸ï¼ŒåŸ·è¡Œæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹
        print("ğŸš€ åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹...")
        print("=" * 60)
        analyzer.run_all_tests()


if __name__ == "__main__":
    main()
