#!/usr/bin/env python3
"""
TCK çµ±ä¸€ç¨‹å¼ç¢¼ç®¡ç†ç³»çµ± (Code Repository Manager)
===============================================

çµ±ä¸€ç®¡ç†æ‰€æœ‰ç¨‹å¼ç¢¼ç‰‡æ®µï¼Œæä¾›åŠŸèƒ½ç›¸ä¼¼æ€§åˆ†æï¼Œ
æ”¯æ´è…³æœ¬é–“çš„è³‡æ–™å‚³éå’Œçµæœç´¯ç©ã€‚

Author: TurboCode Kit (TCK)  
Version: 1.1
"""

import os
import ast
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Union
from dataclasses import dataclass, asdict
from collections import defaultdict
from tqdm import tqdm


@dataclass
class CodeFragment:
    """ç¨‹å¼ç¢¼ç‰‡æ®µè³‡æ–™çµæ§‹"""
    id: str                          # å”¯ä¸€è­˜åˆ¥ç¢¼
    name: str                        # ç‰‡æ®µåç¨±ï¼ˆå‡½æ•¸åã€é¡åç­‰ï¼‰
    type: str                        # ç‰‡æ®µé¡å‹ï¼ˆfunction, class, methodç­‰ï¼‰
    file_path: str                   # æª”æ¡ˆè·¯å¾‘
    start_line: int                  # é–‹å§‹è¡Œè™Ÿ
    end_line: int                    # çµæŸè¡Œè™Ÿ
    raw_code: str                    # åŸå§‹ç¨‹å¼ç¢¼
    normalized_code: str             # æ¨™æº–åŒ–ç¨‹å¼ç¢¼
    ast_structure: str               # ASTçµæ§‹ç°½å
    semantic_signature: str          # èªæ„ç°½å
    hash_code: str                   # ç¨‹å¼ç¢¼é›œæ¹Šå€¼
    
    # åˆ†æçµæœç´¯ç©å€
    frequency_data: Optional[Dict] = None      # é »ç‡åˆ†æçµæœ
    complexity_data: Optional[Dict] = None     # è¤‡é›œåº¦åˆ†æçµæœ
    similarity_data: Optional[Dict] = None     # ç›¸ä¼¼åº¦åˆ†æçµæœ
    
    # åŠŸèƒ½è­˜åˆ¥
    functional_patterns: Optional[List[str]] = None      # åŠŸèƒ½æ¨¡å¼æ¨™ç±¤
    optimization_opportunities: Optional[List[Dict]] = None  # å„ªåŒ–æ©Ÿæœƒ
    
    def __post_init__(self):
        """åˆå§‹åŒ–å¾Œè™•ç†"""
        if self.functional_patterns is None:
            self.functional_patterns = []
        if self.optimization_opportunities is None:
            self.optimization_opportunities = []


class CodeRepository:
    """çµ±ä¸€ç¨‹å¼ç¢¼ç®¡ç†ä¸­å¿ƒ"""
    
    def __init__(self, config_path: str = "config.json"):
        """åˆå§‹åŒ–ç¨‹å¼ç¢¼å€‰åº«"""
        self.config = self._load_config(config_path)
        self.fragments: Dict[str, CodeFragment] = {}
        self.functional_groups: Dict[str, List[str]] = defaultdict(list)
        self.analysis_history: List[Dict] = []
        
        # å„ªåŒ–ï¼šé å»ºåŠŸèƒ½æ¨¡å¼é—œéµå­—é›†åˆ (åŸºæ–¼ loop_lookup_optimization.md A+ç´š 221.0x åŠ é€Ÿ)
        self._sorting_keywords = {'sort', 'sorted'}
        self._searching_keywords = {'find', 'search', 'index'}
        self._aggregation_keywords = {'count', 'sum', 'max', 'min'}
        self._data_processing_keywords = {'filter', 'map', 'reduce'}
        
    def _load_config(self, config_path: str) -> Dict:
        """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        default_config = {
            "global_config": {
                "root_directory": ".",
                "output_directory": "../tck_core/analysis_results",
                "repository_file": "code_repository.json"
            },
            "code_analysis": {
                "min_function_length": 3,
                "extract_classes": True,
                "extract_methods": True,
                "extract_functions": True,
                "functional_pattern_matching": True
            }
        }
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    def scan_and_extract_all(self) -> Dict[str, int]:
        """æƒæä¸¦æå–æ‰€æœ‰ç¨‹å¼ç¢¼ç‰‡æ®µ"""
        print("ğŸ” é–‹å§‹æƒæç¨‹å¼ç¢¼å€‰åº«...")
        
        root_dir = Path(self.config["global_config"]["root_directory"])
        ignore_folders = set(self.config["global_config"]["ignore_folders"])
        
        # å„ªåŒ–ï¼šä½¿ç”¨ç”Ÿæˆå™¨æ”¶é›†æ‰€æœ‰ Python æª”æ¡ˆ
        print("ğŸ“ æ­£åœ¨æ”¶é›† Python æª”æ¡ˆ...")
        py_files = []
        for file_path in root_dir.rglob("*.py"):
            # å„ªåŒ–ï¼šä½¿ç”¨é›†åˆäº¤é›†é€²è¡Œ O(1) æŸ¥æ‰¾ (åŸºæ–¼ loop_lookup_optimization.md A+ç´š)
            file_path_parts = set(file_path.parts)
            if not file_path_parts & ignore_folders:  # O(1) é›†åˆäº¤é›†æ“ä½œ
                py_files.append(file_path)
        
        print(f"âœ¨ æ‰¾åˆ° {len(py_files)} å€‹ Python æª”æ¡ˆ")
        
        stats = {"files": 0, "functions": 0, "classes": 0, "methods": 0}
        
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
        with tqdm(total=len(py_files), desc="ğŸ“„ è™•ç†æª”æ¡ˆ", unit="æª”æ¡ˆ") as pbar:
            for file_path in py_files:
                pbar.set_description(f"ğŸ“„ {file_path.name[:30]}")
                
                file_fragments = self._extract_from_file(str(file_path))
                
                for fragment in file_fragments:
                    self.fragments[fragment.id] = fragment
                    if fragment.type in stats:
                        stats[fragment.type + "s"] = stats.get(fragment.type + "s", 0) + 1
                
                stats["files"] += 1
                pbar.update(1)
        
        print(f"ğŸ“Š ç‰‡æ®µçµ±è¨ˆ: {len(self.fragments)} å€‹ç¨‹å¼ç¢¼ç‰‡æ®µ")
        
        # é€²è¡ŒåŠŸèƒ½æ¨¡å¼åˆ†æ
        self._analyze_functional_patterns()
        
        # ä¿å­˜ç¨‹å¼ç¢¼å€‰åº«
        self._save_repository()
        
        print("âœ… ç¨‹å¼ç¢¼å€‰åº«å»ºç«‹å®Œæˆï¼")
        print(f"   æª”æ¡ˆæ•¸: {stats['files']}")
        print(f"   å‡½æ•¸æ•¸: {stats.get('functions', 0)}")
        print(f"   é¡åˆ¥æ•¸: {stats.get('classs', 0)}")
        print(f"   æ–¹æ³•æ•¸: {stats.get('methods', 0)}")
        
        return stats
    
    def _extract_from_file(self, file_path: str) -> List[CodeFragment]:
        """å¾å–®ä¸€æª”æ¡ˆæå–ç¨‹å¼ç¢¼ç‰‡æ®µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.splitlines()
            
            tree = ast.parse(content, filename=file_path)
            fragments = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    fragment = self._create_fragment_from_function(node, "function", file_path, lines)
                    if fragment:
                        fragments.append(fragment)
                
                elif isinstance(node, ast.ClassDef):
                    fragment = self._create_fragment_from_class(node, "class", file_path, lines)
                    if fragment:
                        fragments.append(fragment)
                        
                        # æå–é¡åˆ¥ä¸­çš„æ–¹æ³•
                        for method_node in node.body:
                            if isinstance(method_node, ast.FunctionDef):
                                method_fragment = self._create_fragment_from_function(
                                    method_node, "method", file_path, lines, parent_class=node.name
                                )
                                if method_fragment:
                                    fragments.append(method_fragment)
            
            return fragments
            
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è™•ç†æª”æ¡ˆ {file_path}: {e}")
            return []
    
    def _create_fragment_from_function(self, node: ast.FunctionDef, fragment_type: str, 
                                     file_path: str, lines: List[str], 
                                     parent_class: Optional[str] = None) -> Optional[CodeFragment]:
        """å¾å‡½æ•¸ç¯€é»å‰µå»ºç¨‹å¼ç¢¼ç‰‡æ®µ"""
        return self._create_fragment_from_ast(node, fragment_type, file_path, lines, parent_class)
    
    def _create_fragment_from_class(self, node: ast.ClassDef, fragment_type: str, 
                                  file_path: str, lines: List[str]) -> Optional[CodeFragment]:
        """å¾é¡åˆ¥ç¯€é»å‰µå»ºç¨‹å¼ç¢¼ç‰‡æ®µ"""
        return self._create_fragment_from_ast(node, fragment_type, file_path, lines)
    
    def _create_fragment_from_ast(self, node: Union[ast.FunctionDef, ast.ClassDef], fragment_type: str, 
                                file_path: str, lines: List[str], 
                                parent_class: Optional[str] = None) -> Optional[CodeFragment]:
        """å¾ AST ç¯€é»å‰µå»ºç¨‹å¼ç¢¼ç‰‡æ®µ"""
        start_line = node.lineno - 1
        end_line = self._find_end_line(node, lines, start_line)
        
        # æª¢æŸ¥é•·åº¦é–¾å€¼
        if end_line - start_line < self.config["code_analysis"]["min_function_length"]:
            return None
        
        raw_code = "\n".join(lines[start_line:end_line])
        normalized_code = self._normalize_code(raw_code)
        
        # ç”Ÿæˆå”¯ä¸€ID
        fragment_id = hashlib.md5(f"{file_path}:{node.name}:{start_line}".encode()).hexdigest()[:12]
        
        # ç”Ÿæˆèªæ„ç°½å
        semantic_sig = self._generate_semantic_signature(node, raw_code)
        
        # ç”ŸæˆASTçµæ§‹ç°½å
        ast_sig = self._generate_ast_signature(node)
        
        fragment_name = f"{parent_class}.{node.name}" if parent_class else node.name
        
        return CodeFragment(
            id=fragment_id,
            name=fragment_name,
            type=fragment_type,
            file_path=file_path,
            start_line=start_line,
            end_line=end_line,
            raw_code=raw_code,
            normalized_code=normalized_code,
            ast_structure=ast_sig,
            semantic_signature=semantic_sig,
            hash_code=hashlib.md5(normalized_code.encode()).hexdigest()
        )
    
    def _find_end_line(self, node: Union[ast.FunctionDef, ast.ClassDef], lines: List[str], start_line: int) -> int:
        """æ‰¾åˆ°ç¨‹å¼ç¢¼ç‰‡æ®µçš„çµæŸè¡Œ"""
        if start_line >= len(lines):
            return len(lines)
        
        # æ‰¾åˆ°èµ·å§‹ç¸®æ’ç´šåˆ¥
        if start_line < len(lines):
            indent_level = len(lines[start_line]) - len(lines[start_line].lstrip())
        else:
            return len(lines)
        
        # å‘ä¸‹æœå°‹ç›´åˆ°æ‰¾åˆ°ç›¸åŒæˆ–æ›´å°‘ç¸®æ’çš„éç©ºè¡Œ
        for i in range(start_line + 1, len(lines)):
            if i >= len(lines):
                break
            line = lines[i].strip()
            if line and not line.startswith(('#', '"', "'")):
                current_indent = len(lines[i]) - len(lines[i].lstrip())
                if current_indent <= indent_level:
                    return i
        
        return len(lines)
    
    def _normalize_code(self, raw_code: str) -> str:
        """æ¨™æº–åŒ–ç¨‹å¼ç¢¼"""
        lines = []
        for line in raw_code.split('\n'):
            # ç§»é™¤è¡Œå…§è¨»é‡‹
            if '#' in line:
                line = line[:line.find('#')]
            # æ¨™æº–åŒ–ç©ºç™½
            line = ' '.join(line.split())
            if line.strip():
                lines.append(line.strip())
        
        return '\n'.join(lines)
    
    def _generate_semantic_signature(self, node: Union[ast.FunctionDef, ast.ClassDef], raw_code: str) -> str:
        """ç”Ÿæˆèªæ„ç°½åï¼Œè­˜åˆ¥åŠŸèƒ½æ¨¡å¼"""
        signature_parts = []
        
        # åˆ†æå‡½æ•¸åƒæ•¸æ¨¡å¼
        if isinstance(node, ast.FunctionDef):
            signature_parts.append(f"params:{len(node.args.args)}")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è¿´åœˆ
            has_loops = any(isinstance(n, (ast.For, ast.While)) for n in ast.walk(node))
            if has_loops:
                signature_parts.append("pattern:loop")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ¢ä»¶åˆ¤æ–·
            has_conditions = any(isinstance(n, ast.If) for n in ast.walk(node))
            if has_conditions:
                signature_parts.append("pattern:condition")
            
            # å„ªåŒ–ï¼šä½¿ç”¨é å»ºé›†åˆé€²è¡Œ O(1) æŸ¥æ‰¾ (åŸºæ–¼ loop_lookup_optimization.md)
            code_lower = raw_code.lower()
            if any(keyword in code_lower for keyword in self._sorting_keywords):
                signature_parts.append("function:sorting")
            if any(keyword in code_lower for keyword in self._searching_keywords):
                signature_parts.append("function:searching")
            if any(keyword in code_lower for keyword in self._aggregation_keywords):
                signature_parts.append("function:aggregation")
            if any(keyword in code_lower for keyword in self._data_processing_keywords):
                signature_parts.append("function:data_processing")
        
        return ":".join(signature_parts)
    
    def _generate_ast_signature(self, node: Union[ast.FunctionDef, ast.ClassDef]) -> str:
        """ç”Ÿæˆ AST çµæ§‹ç°½å"""
        node_types = []
        for child in ast.walk(node):
            node_types.append(type(child).__name__)
        
        # çµ±è¨ˆå„ç¨®ç¯€é»é¡å‹çš„æ•¸é‡
        from collections import Counter
        type_counts = Counter(node_types)
        
        # ç”Ÿæˆçµæ§‹ç°½å
        signature = []
        for node_type in ['For', 'While', 'If', 'Call', 'Return']:
            if node_type in type_counts:
                signature.append(f"{node_type}:{type_counts[node_type]}")
        
        return ":".join(signature)
    
    def _analyze_functional_patterns(self):
        """åˆ†æåŠŸèƒ½æ¨¡å¼ï¼Œè­˜åˆ¥åŠŸèƒ½ç›¸ä¼¼çš„ç¨‹å¼ç¢¼"""
        print("ğŸ”„ åˆ†æåŠŸèƒ½æ¨¡å¼...")
        
        # æŒ‰èªæ„ç°½ååˆ†çµ„
        semantic_groups = defaultdict(list)
        for fragment in self.fragments.values():
            # ç¢ºä¿ functional_patterns å·²åˆå§‹åŒ–
            if fragment.functional_patterns is None:
                fragment.functional_patterns = []
                
            # æå–ä¸»è¦åŠŸèƒ½æ¨¡å¼
            main_pattern = self._extract_main_pattern(fragment.semantic_signature)
            if main_pattern:
                semantic_groups[main_pattern].append(fragment.id)
                fragment.functional_patterns.append(main_pattern)
        
        # è¨˜éŒ„åŠŸèƒ½åˆ†çµ„
        for pattern, fragment_ids in semantic_groups.items():
            if len(fragment_ids) > 1:  # åªè¨˜éŒ„æœ‰å¤šå€‹å¯¦ç¾çš„åŠŸèƒ½
                self.functional_groups[pattern] = fragment_ids
                print(f"  ğŸ”— ç™¼ç¾åŠŸèƒ½æ¨¡å¼ '{pattern}': {len(fragment_ids)} å€‹å¯¦ç¾")
    
    def _extract_main_pattern(self, semantic_signature: str) -> Optional[str]:
        """æå–ä¸»è¦åŠŸèƒ½æ¨¡å¼"""
        if not semantic_signature:
            return None
        
        parts = semantic_signature.split(':')
        
        # å„ªå…ˆé¸æ“‡åŠŸèƒ½é¡å‹
        for part in parts:
            if part.startswith('function:'):
                return part.replace('function:', '')
        
        # å…¶æ¬¡é¸æ“‡æ¨¡å¼é¡å‹
        for part in parts:
            if part.startswith('pattern:'):
                return part.replace('pattern:', '')
        
        return None
    
    def get_fragment(self, fragment_id: str) -> Optional[CodeFragment]:
        """ç²å–ç¨‹å¼ç¢¼ç‰‡æ®µ"""
        return self.fragments.get(fragment_id)
    
    def get_all_fragments(self) -> Dict[str, CodeFragment]:
        """ç²å–æ‰€æœ‰ç¨‹å¼ç¢¼ç‰‡æ®µ"""
        return self.fragments
    
    def update_fragment_analysis(self, fragment_id: str, analysis_type: str, data: Dict):
        """æ›´æ–°ç¨‹å¼ç¢¼ç‰‡æ®µçš„åˆ†æçµæœ"""
        if fragment_id in self.fragments:
            fragment = self.fragments[fragment_id]
            if analysis_type == "frequency":
                fragment.frequency_data = data
            elif analysis_type == "complexity":
                fragment.complexity_data = data
            elif analysis_type == "similarity":
                fragment.similarity_data = data
    
    def get_functional_groups(self) -> Dict[str, List[Dict]]:
        """ç²å–åŠŸèƒ½åˆ†çµ„ï¼ŒåŒ…å«å®Œæ•´çš„ç¨‹å¼ç¢¼ç‰‡æ®µè³‡è¨Š"""
        groups = {}
        for pattern, fragment_ids in self.functional_groups.items():
            groups[pattern] = [
                {
                    "id": fid,
                    "name": self.fragments[fid].name,
                    "file": os.path.basename(self.fragments[fid].file_path),
                    "lines": f"{self.fragments[fid].start_line+1}-{self.fragments[fid].end_line}",
                    "raw_code": self.fragments[fid].raw_code[:200] + "..." if len(self.fragments[fid].raw_code) > 200 else self.fragments[fid].raw_code
                }
                for fid in fragment_ids if fid in self.fragments
            ]
        return groups
    
    def _save_repository(self):
        """ä¿å­˜ç¨‹å¼ç¢¼å€‰åº«"""
        output_dir = Path(self.config["global_config"]["output_directory"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        repo_data = {
            "metadata": {
                "total_fragments": len(self.fragments),
                "functional_groups": len(self.functional_groups)
            },
            "fragments": {fid: asdict(fragment) for fid, fragment in self.fragments.items()},
            "functional_groups": dict(self.functional_groups),
            "analysis_history": self.analysis_history
        }
        
        repo_file = output_dir / self.config["global_config"]["repository_file"]
        with open(repo_file, 'w', encoding='utf-8') as f:
            json.dump(repo_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ç¨‹å¼ç¢¼å€‰åº«å·²ä¿å­˜: {repo_file}")
    
    def load_repository(self) -> bool:
        """è¼‰å…¥ç¨‹å¼ç¢¼å€‰åº«"""
        output_dir = Path(self.config["global_config"]["output_directory"])
        repo_file = output_dir / self.config["global_config"]["repository_file"]
        
        if not repo_file.exists():
            return False
        
        try:
            with open(repo_file, 'r', encoding='utf-8') as f:
                repo_data = json.load(f)
            
            # é‡å»ºç¨‹å¼ç¢¼ç‰‡æ®µ
            self.fragments = {}
            for fid, fragment_data in repo_data["fragments"].items():
                fragment = CodeFragment(**fragment_data)
                self.fragments[fid] = fragment
            
            self.functional_groups = defaultdict(list, repo_data["functional_groups"])
            self.analysis_history = repo_data.get("analysis_history", [])
            
            print(f"âœ… ç¨‹å¼ç¢¼å€‰åº«å·²è¼‰å…¥: {len(self.fragments)} å€‹ç‰‡æ®µ")
            return True
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥ç¨‹å¼ç¢¼å€‰åº«å¤±æ•—: {e}")
            return False


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print("ğŸš€ TCK ç¨‹å¼ç¢¼å€‰åº«ç®¡ç†ç³»çµ±")
    
    repo = CodeRepository()
    
    # å˜—è©¦è¼‰å…¥ç¾æœ‰å€‰åº«
    if repo.load_repository():
        print("ç™¼ç¾ç¾æœ‰ç¨‹å¼ç¢¼å€‰åº«")
        choice = input("æ˜¯å¦é‡æ–°æƒæï¼Ÿ(y/n): ").lower().strip()
        if choice in ['y', 'yes', 'æ˜¯']:
            repo.scan_and_extract_all()
    else:
        print("å»ºç«‹æ–°çš„ç¨‹å¼ç¢¼å€‰åº«")
        repo.scan_and_extract_all()
    
    # é¡¯ç¤ºåŠŸèƒ½åˆ†çµ„æ‘˜è¦
    functional_groups = repo.get_functional_groups()
    if functional_groups:
        print("\nğŸ”— ç™¼ç¾çš„åŠŸèƒ½æ¨¡å¼:")
        for pattern, implementations in functional_groups.items():
            print(f"  ğŸ“‹ {pattern}: {len(implementations)} å€‹ä¸åŒå¯¦ç¾")
            for impl in implementations[:3]:  # é¡¯ç¤ºå‰3å€‹
                print(f"    - {impl['name']} ({impl['file']})")


if __name__ == "__main__":
    main()