"""
TurboCode Kit (TCK) - ç›¸ä¼¼åº¦æª¢æ¸¬å™¨
ç”¨æ–¼æª¢æ¸¬ç¨‹å¼ç¢¼ä¸­çš„é‡è¤‡å’Œç›¸ä¼¼æ¨¡å¼
"""

import ast
import hashlib
import json
import os
from collections import defaultdict
from difflib import SequenceMatcher
from typing import Dict, List

from simhash import Simhash, SimhashIndex
from tqdm import tqdm


class SimilarityDetector:
    """ç¨‹å¼ç¢¼ç›¸ä¼¼åº¦æª¢æ¸¬å™¨"""

    def __init__(self, config_path: str = "config.json"):
        """åˆå§‹åŒ–ç›¸ä¼¼åº¦æª¢æ¸¬å™¨"""
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = json.load(f)

        self.code_blocks = []
        self.function_signatures = defaultdict(list)
        # å„ªåŒ–ï¼šé å…ˆå°‡æ’é™¤ç›®éŒ„è½‰æ›ç‚ºé›†åˆï¼Œå¯¦ç¾ O(1) æŸ¥æ‰¾
        self.exclude_dirs_set = set(self.config["scan_settings"]["exclude_dirs"])

    def scan_directory(self, directory: str) -> List[Dict]:
        """æƒæç›®éŒ„ä¸­çš„æ‰€æœ‰ Python æª”æ¡ˆ"""
        python_files = []

        print(f"ğŸ” æ­£åœ¨æƒæç›®éŒ„: {directory}")

        for root, dirs, files in os.walk(directory):
            # å„ªåŒ–ï¼šä½¿ç”¨é å»ºé›†åˆé€²è¡Œ O(1) æŸ¥æ‰¾è€Œä¸æ˜¯ O(n) åˆ—è¡¨æŸ¥æ‰¾
            dirs[:] = [d for d in dirs if d not in self.exclude_dirs_set]

            for file in files:
                if file.endswith(".py"):
                    file_path = os.path.join(root, file)
                    python_files.append(file_path)

        print(f"ğŸ“ æ‰¾åˆ° {len(python_files)} å€‹ Python æª”æ¡ˆ")

        # è™•ç†æ¯å€‹æª”æ¡ˆ
        all_blocks = []
        with tqdm(total=len(python_files), desc="åˆ†ææª”æ¡ˆ", unit="å€‹") as pbar:
            for file_path in python_files:
                blocks = self._extract_code_blocks(file_path)
                all_blocks.extend(blocks)
                pbar.update(1)

        self.code_blocks = all_blocks
        print(f"ğŸ“Š ç¸½å…±æå–äº† {len(all_blocks)} å€‹ç¨‹å¼ç¢¼å¡Š")

        return all_blocks

    def _extract_code_blocks(self, file_path: str) -> List[Dict]:
        """å¾æª”æ¡ˆä¸­æå–ç¨‹å¼ç¢¼å¡Š"""
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as file:
                content = file.read()
                lines = content.split("\n")

            tree = ast.parse(content, filename=file_path)
            code_blocks = []

            for node in ast.walk(tree):
                if isinstance(
                    node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                ):
                    start_line = node.lineno
                    end_line = self._find_end_line(node, lines)

                    if (
                        end_line - start_line
                        < self.config["complexity_settings"]["min_lines"]
                    ):
                        continue

                    # æå–ç¨‹å¼ç¢¼ç‰‡æ®µ
                    code_lines = lines[start_line - 1 : end_line]
                    raw_code = "\n".join(code_lines)
                    normalized_code = self._normalize_code(code_lines)

                    if len(normalized_code.strip()) == 0:
                        continue

                    # å»ºç«‹ç¨‹å¼ç¢¼å¡Šè³‡è¨Š
                    code_block = {
                        "file_path": file_path,
                        "start_line": start_line,
                        "end_line": end_line,
                        "raw_code": raw_code,
                        "normalized_code": normalized_code,
                        "type": type(node).__name__,
                        "name": getattr(node, "name", "unknown"),
                        "signature": self._get_function_signature(node)
                        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))
                        else None,
                    }

                    # ç”Ÿæˆç¨‹å¼ç¢¼é›œæ¹Š
                    code_block["hash"] = hashlib.md5(
                        code_block["normalized_code"].encode()
                    ).hexdigest()

                    # ç”Ÿæˆ SimHash æŒ‡ç´‹ç”¨æ–¼å¿«é€Ÿç›¸ä¼¼åº¦æª¢æ¸¬
                    code_block["simhash"] = Simhash(code_block["normalized_code"]).value

                    code_blocks.append(code_block)

                    # è¨˜éŒ„å‡½æ•¸ç°½å
                    if code_block["signature"]:
                        self.function_signatures[code_block["signature"]].append(
                            code_block
                        )

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è™•ç†æª”æ¡ˆ {file_path}: {e}")
            return []

        return code_blocks

    def _find_end_line(self, node: ast.stmt, lines: List[str]) -> int:
        """æ‰¾åˆ° AST ç¯€é»çš„çµæŸè¡Œ"""
        # ç°¡å–®çš„å•Ÿç™¼å¼æ–¹æ³•ï¼šæ‰¾åˆ°ä¸‹ä¸€å€‹åŒç´šå®šç¾©æˆ–æª”æ¡ˆçµæŸ
        start_line = node.lineno - 1

        # æ‰¾åˆ°å‡½æ•¸çš„ç¸®æ’ç´šåˆ¥
        if start_line < len(lines):
            indent_level = len(lines[start_line]) - len(lines[start_line].lstrip())
        else:
            return len(lines)

        # å‘ä¸‹æœå°‹ï¼Œç›´åˆ°æ‰¾åˆ°ç›¸åŒæˆ–æ›´å°‘ç¸®æ’çš„éç©ºè¡Œ
        for i in range(start_line + 1, len(lines)):
            line = lines[i].strip()
            if line:  # éç©ºè¡Œ
                current_indent = len(lines[i]) - len(lines[i].lstrip())
                if current_indent <= indent_level and not line.startswith(
                    ("#", '"', "'")
                ):
                    return i

        return len(lines)

    def _normalize_code(self, code_lines: List[str]) -> str:
        """æ¨™æº–åŒ–ç¨‹å¼ç¢¼ï¼ˆç§»é™¤è¨»é‡‹ã€ç©ºç™½ç­‰ï¼‰"""
        normalized_lines = []

        for line in code_lines:
            # ç§»é™¤è¨»é‡‹
            if self.config["similarity_settings"]["ignore_comments"]:
                if "#" in line:
                    line = line[: line.find("#")]

            # æ¨™æº–åŒ–ç©ºç™½
            if self.config["similarity_settings"]["ignore_whitespace"]:
                line = " ".join(line.split())

            if line.strip():
                normalized_lines.append(line.strip())

        return "\n".join(normalized_lines)

    def _get_function_signature(
        self, func_node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> str:
        """æå–å‡½æ•¸ç°½åï¼ˆåƒæ•¸é¡å‹å’Œæ•¸é‡ï¼‰"""
        params = []
        for arg in func_node.args.args:
            params.append(arg.arg)

        return f"{func_node.name}({len(params)})"

    def calculate_similarity(self, code1: str, code2: str) -> float:
        """è¨ˆç®—å…©æ®µç¨‹å¼ç¢¼çš„ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, code1, code2).ratio()

    def find_similar_blocks_parallel(self) -> List[Dict]:
        """ä½¿ç”¨ SimHash é éæ¿¾çš„é«˜æ•ˆç›¸ä¼¼åº¦æª¢æ¸¬ (æ¥è¿‘ O(N) è¤‡é›œåº¦)"""
        similar_groups = []
        processed_hashes = set()
        # ä½¿ç”¨é…ç½®ä¸­çš„ç›¸ä¼¼åº¦é–¾å€¼é€²è¡Œæª¢æ¸¬
        config_threshold = self.config["similarity_settings"]["similarity_threshold"]
        simhash_threshold = 3  # Hamming è·é›¢é–¾å€¼ï¼Œå¯èª¿æ•´

        print("ğŸ”„ æ­£åœ¨ä½¿ç”¨ SimHash é éæ¿¾é€²è¡Œç¨‹å¼ç¢¼ç›¸ä¼¼åº¦åˆ†æ...")

        # å…ˆæŒ‰é›œæ¹Šåˆ†çµ„ï¼Œè™•ç†å®Œå…¨é‡è¤‡çš„ç¨‹å¼ç¢¼
        hash_groups = defaultdict(list)
        for block in self.code_blocks:
            hash_groups[block["hash"]].append(block)

        # è™•ç†å®Œå…¨é‡è¤‡çš„ç¨‹å¼ç¢¼ï¼ˆé›œæ¹Šç›¸åŒï¼‰
        exact_duplicates = []
        remaining_blocks = []

        for hash_code, blocks in hash_groups.items():
            if len(blocks) > 1:
                # å®Œå…¨é‡è¤‡çš„ç¨‹å¼ç¢¼
                group = {
                    "group_id": len(exact_duplicates) + 1,
                    "similarity_type": "exact",
                    "block_count": len(blocks),
                    "blocks": blocks,
                    "optimization_potential": self._calculate_optimization_potential(
                        blocks
                    ),
                }
                exact_duplicates.append(group)
                processed_hashes.add(hash_code)
            else:
                remaining_blocks.extend(blocks)

        similar_groups.extend(exact_duplicates)
        print(f"âœ… æ‰¾åˆ° {len(exact_duplicates)} å€‹å®Œå…¨é‡è¤‡çš„ç¨‹å¼ç¢¼çµ„")

        # ä½¿ç”¨ SimHash ç´¢å¼•é€²è¡Œé«˜æ•ˆç›¸ä¼¼åº¦æª¢æ¸¬
        if remaining_blocks:
            try:
                print(
                    f"ğŸ”„ å»ºç«‹ SimHash ç´¢å¼•ç”¨æ–¼ {len(remaining_blocks)} å€‹ç¨‹å¼ç¢¼å¡Šçš„å¿«é€ŸæŸ¥æ‰¾..."
                )

                # å»ºç«‹ SimHash ç´¢å¼•ï¼ˆç”¨æ–¼ç›¸ä¼¼åº¦æª¢æ¸¬ï¼‰
                simhash_objs = [
                    (block["hash"], Simhash(block["normalized_code"]))
                    for block in remaining_blocks
                ]
                index = SimhashIndex(simhash_objs, k=simhash_threshold)

                # ä½¿ç”¨é…ç½®é–¾å€¼é€²è¡Œç›¸ä¼¼åº¦æª¢æ¸¬
                for i, block in enumerate(remaining_blocks):
                    if block["hash"] not in processed_hashes:
                        similar_hashes = index.get_near_dups(
                            Simhash(block["normalized_code"])
                        )
                        if len(similar_hashes) > 1:  # åŒ…å«è‡ªå·±
                            group_blocks = [
                                b
                                for b in remaining_blocks
                                if b["hash"] in similar_hashes
                                and self.calculate_similarity(
                                    block["normalized_code"], b["normalized_code"]
                                )
                                >= config_threshold
                            ]
                            if len(group_blocks) > 1:
                                similar_groups.append(group_blocks)
                                processed_hashes.update(b["hash"] for b in group_blocks)

            except Exception as e:
                print(
                    f"âš ï¸ SimHash ç´¢å¼•å¤±æ•—: {e}ï¼Œè·³éç›¸ä¼¼åº¦æª¢æ¸¬ï¼Œåªè¿”å›å®Œå…¨é‡è¤‡çš„çµæœ..."
                )
                # ä¸é€²è¡Œåºåˆ—ç›¸ä¼¼åº¦æª¢æ¸¬ï¼Œç›´æ¥è¿”å›å®Œå…¨é‡è¤‡çš„çµæœ
                pass

        print("ğŸš€ SimHash å„ªåŒ–å®Œæˆï¼å¾ O(NÂ²) é™ç´šç‚ºæ¥è¿‘ O(N) çš„æŸ¥æ‰¾æ•ˆç‡")
        return similar_groups

    def _find_similar_blocks_sequential(
        self, blocks: List[Dict], processed_hashes: set
    ) -> List[Dict]:
        """åºåˆ—æ–¹å¼æ‰¾å‡ºç›¸ä¼¼çš„ç¨‹å¼ç¢¼å¡Šï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""
        similar_groups = []
        similarity_threshold = self.config["similarity_settings"][
            "similarity_threshold"
        ]
        min_code_length = self.config["similarity_settings"]["min_code_length"]

        # é éæ¿¾ï¼šåªè™•ç†è¶³å¤ é•·çš„ç¨‹å¼ç¢¼å¡Š
        filtered_blocks = [
            block
            for block in blocks
            if len(block["normalized_code"]) >= min_code_length
        ]
        print(f"ğŸ” éæ¿¾å¾Œå‰©é¤˜ {len(filtered_blocks)} å€‹ç¨‹å¼ç¢¼å¡Šé€²è¡Œç›¸ä¼¼åº¦åˆ†æ")

        with tqdm(total=len(filtered_blocks), desc="åºåˆ—ç›¸ä¼¼åº¦æª¢æ¸¬", unit="å€‹") as pbar:
            for i, block1 in enumerate(filtered_blocks):
                if block1["hash"] in processed_hashes:
                    pbar.update(1)
                    continue

                # é€²ä¸€æ­¥éæ¿¾ï¼šé•·åº¦ç›¸ä¼¼çš„ç¨‹å¼ç¢¼å¡Š
                similar_candidates = []
                len1 = len(block1["normalized_code"])

                for block2 in filtered_blocks[i + 1 :]:
                    if block2["hash"] in processed_hashes:
                        continue

                    len2 = len(block2["normalized_code"])
                    # é•·åº¦ç›¸ä¼¼åº¦æª¢æŸ¥ï¼ˆå…è¨±30%çš„å·®ç•°ï¼‰
                    if abs(len1 - len2) / max(len1, len2) <= 0.3:
                        similar_candidates.append(block2)

                # åªå°å€™é¸è€…é€²è¡Œè©³ç´°ç›¸ä¼¼åº¦è¨ˆç®—
                similar_blocks = []
                for block2 in similar_candidates:
                    if block2["hash"] in processed_hashes:
                        continue

                    similarity = self.calculate_similarity(
                        block1["normalized_code"], block2["normalized_code"]
                    )

                    if similarity >= similarity_threshold:
                        similar_blocks.append(block2)
                        processed_hashes.add(block2["hash"])

                if similar_blocks:
                    group = {
                        "group_id": len(similar_groups) + 1,
                        "similarity_type": "similar",
                        "block_count": len(similar_blocks) + 1,
                        "blocks": [block1] + similar_blocks,
                        "optimization_potential": self._calculate_optimization_potential(
                            [block1] + similar_blocks
                        ),
                    }
                    similar_groups.append(group)

                processed_hashes.add(block1["hash"])
                pbar.update(1)

        return similar_groups

    def _calculate_optimization_potential(self, blocks: List[Dict]) -> Dict:
        """è¨ˆç®—å„ªåŒ–æ½›åŠ›"""
        if not blocks:
            return {
                "duplicate_lines": 0,
                "potential_savings": "0 è¡Œç¨‹å¼ç¢¼",
                "refactor_suggestion": "ç„¡",
            }

        # è¨ˆç®—é‡è¤‡çš„è¡Œæ•¸
        first_block = blocks[0]
        total_lines = sum(block["end_line"] - block["start_line"] for block in blocks)
        duplicate_lines = total_lines - (
            first_block["end_line"] - first_block["start_line"]
        )

        # ç”Ÿæˆå»ºè­°
        if duplicate_lines > 20:
            suggestion = "å¼·çƒˆå»ºè­°é‡æ§‹ï¼šå»ºç«‹å…±ç”¨å‡½æ•¸æˆ–æ¨¡çµ„"
        elif duplicate_lines > 10:
            suggestion = "è€ƒæ…®é‡æ§‹ï¼šæå–å…±åŒé‚è¼¯"
        elif duplicate_lines > 5:
            suggestion = "å¯ä»¥è€ƒæ…®åˆä½µç›¸ä¼¼ç¨‹å¼ç¢¼"
        else:
            suggestion = "å°é‡é‡è¤‡ï¼Œå¯ä»¥ä¿æŒç¾ç‹€"

        return {
            "duplicate_lines": duplicate_lines,
            "potential_savings": f"{duplicate_lines} è¡Œç¨‹å¼ç¢¼",
            "refactor_suggestion": suggestion,
        }

    def run_analysis(self) -> Dict:
        """åŸ·è¡Œç›¸ä¼¼åº¦åˆ†æ"""
        if not self.code_blocks:
            print("âš ï¸ å°šæœªæƒæä»»ä½•ç¨‹å¼ç¢¼ï¼Œè«‹å…ˆåŸ·è¡Œ scan_directory()")
            return {}

        print("ğŸš€ é–‹å§‹ç›¸ä¼¼åº¦åˆ†æ...")

        # ä½¿ç”¨ä¸¦è¡Œè™•ç†æ‰¾å‡ºç›¸ä¼¼çš„ç¨‹å¼ç¢¼å¡Š
        similar_groups = self.find_similar_blocks_parallel()

        # çµ±è¨ˆçµæœ
        total_duplicates = sum(group["block_count"] for group in similar_groups)
        total_savings = sum(
            group["optimization_potential"]["duplicate_lines"]
            for group in similar_groups
        )

        results = {
            "summary": {
                "total_code_blocks": len(self.code_blocks),
                "similar_groups": len(similar_groups),
                "total_duplicate_blocks": total_duplicates,
                "potential_line_savings": total_savings,
                "files_analyzed": len(
                    set(block["file_path"] for block in self.code_blocks)
                ),
            },
            "similar_groups": similar_groups,
        }

        print("âœ… åˆ†æå®Œæˆï¼")
        print(f"   - åˆ†æäº† {results['summary']['total_code_blocks']} å€‹ç¨‹å¼ç¢¼å¡Š")
        print(f"   - æ‰¾åˆ° {results['summary']['similar_groups']} å€‹ç›¸ä¼¼ç¾¤çµ„")
        print(f"   - å¯èƒ½ç¯€çœ {results['summary']['potential_line_savings']} è¡Œç¨‹å¼ç¢¼")

        return results

    def _analyze_function_patterns(self) -> Dict:
        """åˆ†æå‡½æ•¸ç°½åæ¨¡å¼"""
        patterns = {
            "most_common_signatures": [],
            "signature_distribution": {},
            "potential_overloads": [],
        }

        # çµ±è¨ˆå‡½æ•¸ç°½å
        signature_counts = defaultdict(int)
        for signature, blocks in self.function_signatures.items():
            signature_counts[signature] = len(blocks)

        # æ‰¾å‡ºæœ€å¸¸è¦‹çš„ç°½å
        sorted_signatures = sorted(
            signature_counts.items(), key=lambda x: x[1], reverse=True
        )
        patterns["most_common_signatures"] = sorted_signatures[:10]
        patterns["signature_distribution"] = dict(signature_counts)

        # æ‰¾å‡ºå¯èƒ½çš„é‡è¼‰å‡½æ•¸ï¼ˆç›¸åŒå‡½æ•¸åä½†ä¸åŒåƒæ•¸æ•¸é‡ï¼‰
        function_names = defaultdict(list)
        for signature in signature_counts:
            if "(" in signature:
                name = signature.split("(")[0]
                param_count = signature.split("(")[1].split(")")[0]
                function_names[name].append((signature, param_count))

        for name, signatures in function_names.items():
            if len(signatures) > 1:
                patterns["potential_overloads"].append(
                    {"function_name": name, "signatures": signatures}
                )

        return patterns

    def save_results(self, results: Dict, output_file: str = "similarity_results.json"):
        """å„²å­˜åˆ†æçµæœ"""
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ° {output_file}")


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    detector = SimilarityDetector()

    # å¾è¨­å®šæª”è®€å–æ ¹ç›®éŒ„
    with open("config.json", "r", encoding="utf-8") as f:
        config = json.load(f)

    root_dir = config["scan_settings"]["root_directory"]

    # æƒæä¸¦åˆ†æ
    detector.scan_directory(root_dir)
    results = detector.run_analysis()
    detector.save_results(results)

    print("ğŸ‰ ç›¸ä¼¼åº¦æª¢æ¸¬å®Œæˆï¼")
